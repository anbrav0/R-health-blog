[
  {
    "objectID": "GLM_for_count.html",
    "href": "GLM_for_count.html",
    "title": "Genalized Linear Model: Using Count Data",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(katex)"
  },
  {
    "objectID": "GLM_for_count.html#libraries-used",
    "href": "GLM_for_count.html#libraries-used",
    "title": "Genalized Linear Model: Using Count Data",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(katex)"
  },
  {
    "objectID": "GLM_for_count.html#introduction",
    "href": "GLM_for_count.html#introduction",
    "title": "Genalized Linear Model: Using Count Data",
    "section": "2 Introduction",
    "text": "2 Introduction\nIn this presentation, we will be discussing how to use the Generalized Linear Model (GLM) method using count data in the R programming language. We will show how to clean and wrangle the data, show necessary columns to execute our method, discuss assumptions held, showcase the code used to run this GLM, and the interpretation of our output results."
  },
  {
    "objectID": "GLM_for_count.html#what-is-a-glm",
    "href": "GLM_for_count.html#what-is-a-glm",
    "title": "Genalized Linear Model: Using Count Data",
    "section": "3 What is a GLM?",
    "text": "3 What is a GLM?\n\n3.1 lets start with a quick overview of the simple linear regression\nAs you know, a simple linear regression has two major components: a Y, dependent outcome and an X, which is your independent or your predictor variable. the model looks something like this:\nE(Y|X) = \\beta_0 + \\beta_1 where \\beta_0 is your intercept and \\beta_1 is your slope. A linear model is a function, that us used to fit a data. We often use this method to see the association, or strength in association between two variables of interest:\n\n\nCode\n# load data:\ndata(trees)\n\n#rename variables:\nnames(trees) &lt;- c(\"DBH_in\",\"height_ft\", \"volume_ft3\")\n\n# simple model:\nmodel &lt;- lm(DBH_in ~ height_ft, data = trees)\n\n# plot:\nsimple_trees &lt;-\n  #data\n  ggplot(data = trees) +\n  \n  # x and y\n  aes(x = height_ft, y = DBH_in) + \n  \n  #labels\n  labs(title = \"Example of Simple Association - Using Trees Data\",\n       x = \"Height in feet\",\n       y = \"Diameter in inches\") + \n  \n  # add points\n  geom_point() + \n  \n  # add the lm\n  geom_smooth(method = \"lm\", color = \"blue\", se = FALSE) +\n  \n  # add a simple theme\n  theme_bw()\n\n# actual plot: \nsimple_trees\n\n\n\n\n\nOften you will find this model writen in this form:\n E(Y|X) = \\beta_0 + \\beta_1 + \\varepsilon where \\beta_0 and \\beta_1 are our coefficients that need to be estimated and $ $ used for more complex lines Our goal is to see the association between an outcome with an exposure.\n\n\n3.2 Assumptions of a linear question\nBefore diving into the generalized models, lets quickly overview the assumptions of linear regression.\n\nAssumption 1: for each combination of independent variable x, y is a random variable with a certain probability distribution.\nAssumption 2: Y values are statistical dependent.\nAssumption 3: the mean value of Y, for each specific combination of values of X, is a linear function.\nAssumption 4: the variance of Y, is the same for any fixed value of X_n\nAssumption 5: for any fixed combination of X_n, Y is normally distributed.\n\nSomething to note, that in the simple explanation above we are assuming our Y variable (remember one of the points we talked above above? that y holds a specific distribution!) is continuous. So lets talk about our Y variable having a count distribution.\n\n\n3.3 lets talk about the generalized linear model\nthe term “generalized” is a big umbrella term used to describe a large class of models. Our response variable y_i is following an exponential family distribution with a mean of u_i which is sometimes non-linear! However McCallagh and Nelder considered them to be linear because our covariate affect the distribution of y_i only through linear combination.\nthere are three major components of a GLM:\n\nA Random component: which specifies the probability distribution of our response variable\nSystematic component: specifies the explanatory variable (x_1 .. x_n) in our model. Or their linear combination (\\beta_0 + \\beta_1) etc.\na link function: specifies the LINK between the random and systematic components. This helps us figure out our expected values of the response is related to the linear combination of our explanatory variables. for example the link function g(u) = u, which is called an identity function. this models the mean directly. Or, in the case we will talk about today the log of the mean:\n\n log(\\pmb{\\mu}) = \\alpha + \\beta_1x_1 + ... B_nx_n + \\epsilon\n\n\n3.4 What is a Poisson Regression?\na Poisson regression models how the mean of a discrete (or we can say count too!) response variable Y depends on our explanatory X variables. Here is a simple look at the Poisson regression:\n  log \\lambda_i = \\beta_0 + \\beta x_i  where the random component: the distribution of Y is the mean of \\lambda and the systematic component is the explanatory variable (or your X variables, which can be continuous or categorical) that is linearly associated. Or can be transformed if non-linear, and the link function is the log link stated in the section above (link the section number here?)\nAn advantage of using GLM over a normal line model is the link function gives us more flexibility in modeling and this model uses the Maximum likelihood estimate. Additionally we can use different inference tools like Wald’s test for logistic and Poisson models.\n\n\n3.5 important points of Poisson models\n\nwe can use this type of link function by using count data. count data can be describes at the number of devices that can access the internet, number of sex partners you have in your lifetime, and the number of individuals infected with a disease.\nPoisson is unimodel and skewed to the right, both the mean and the variance are the same. In other words, when the count is larger it tends to be more varied.\nif our mew increases the skew decreases and the distribution starts to become more bell shapped. our mean tends to look something like this:\n\n\\pmb{\\mu} = exp(\\alpha + \\beta x) = e^\\alpha (e^\\beta)^x  where one unit increase in X has a multiplicative impact on your e^\\beta power on the mean. (More on this a little later in the interpretation section!)\n\n\n3.6 one last point before we move on\nwhen your modeling count data, the link scale is linear. So the effects are additive on the link. While your response scale is nonlinear (this is on the exponent) and so the effects are multiplicative. makes sense? we will work out an example now!"
  },
  {
    "objectID": "GLM_for_count.html#example-1-using-glm-for-count-responses---crab-data",
    "href": "GLM_for_count.html#example-1-using-glm-for-count-responses---crab-data",
    "title": "Genalized Linear Model: Using Count Data",
    "section": "4 Example 1: using GLM for count responses - Crab Data",
    "text": "4 Example 1: using GLM for count responses - Crab Data\nLets import the crabs.txt data from the University of Florida’s open-source data files."
  },
  {
    "objectID": "Two_sample_t.html",
    "href": "Two_sample_t.html",
    "title": "Two sample t test",
    "section": "",
    "text": "This is also called the independent sample t test. It is used to see whether the unknown population means of two groups are equal or different. This test requires one variable which can be the exposure x and another variable which can be the outcome y. If you have more than two groups then analysis of variance (ANOVA) will be more suitable. If data is nonparametric then an alternative test to use would be the Mann Whitney U test or a permutation test.[@cressie1986].\nThere are two types of t tests, the first being the Student’s t test, which assumes the variance of the two groups is equal, the second being the Welch’s t test (default in R), which assumes the variance in the two groups is different.\nIn this article we will be discussing the Student’s t test.\n\n\n\nMeasurements for one observation do not affect measurements for any other observation (assumes independence).\nData in each group must be obtained via a random sample from the population.\nData in each group are normally distributed.\nData values are continuous.\nThe variances for the two independent groups are equal in the Student’s t test.\nThere should be no significant outliers.\n\n\n\n\n\n(H_0): the mean of group A (m_A) is equal to the mean of group B (m_B)- two tailed test.\n(H_0): the mean of group A (m_A) is greater than or equal to mean of group B (m_B)- one tailed test.\n(H_0): the mean of group A (m_A) is less than or equal to the mean of group B (m_B)- one tailed test.\nThe corresponding alternative hypotheses would be as follows:\n\n\n\n(H_1): the mean of group A (m_A) is different to the mean of group B (m_B)- two tailed test.\n(H_1): the mean of group A (m_A) is less than the mean of group B (m_B)- one tailed test.\n(H_1): the mean of group A (m_A) is greater the mean of group B (m_B)- one tailed test.\n\n\n\n\nFor the Student’s t test which assumes equal variance the following is how the |t| statistic may be calculated using groups A and B as examples:\nt ={ {\\bar{x}_{1} - \\bar{x}_{2}} \\over \\sqrt{ s^2( {1 \\over n_1 } + {1 \\over n_2}) }}\nThis can be described as the sample mean difference divided by the sample standard deviation of the sample mean difference where:\nm_A and m_B are the mean values of A and B,\nn_A and n_B are the seize of group A and B,\nS^2 is the estimator for the pooled variance,\nwith the degrees of freedom (df) = n_A + n_B - 2,\nand s^2 is calculated as follows:\ns^2 = { {\\displaystyle\\sum_{i=1}^{n_1} { (x_i-\\bar{x}_1)^2} + \\displaystyle\\sum_{j=1}^{n_2}{ (x_j-\\bar{x}_2)^2}} \\over {n_{1} + n_{2} - 2 }}\nResults for both Students t test and Welch’s t test are usually similar unless the group sizes and standard deviations are different.\nWhat if the data is not independent?\nIf the data is not independent such as paired data in the form of matched pairs which are correlated, we use the paired t test. This test checks whether the means of two paired groups are different from each other. It’s usually used in clinical trial studies with a “before and after” or case control studies with matched pairs. For this test we only assume the difference of each pair to be normally distributed (the paired groups are the ones important for analysis) unlike the independent t test which assumes that data from both samples are independent and variances are equal.[@fralick]\n\n\n\n\n\n\n\ntidyverse: data manipulation and visualization.\nrstatix: providing pipe friendly R functions for easy statistical analyses.\ncar: providing variance tests.\n\n\n\nCode\n#install.packages(\"ggstatplot\") \n#install.packages(\"car\")\n#install.packages(\"rstatix\")\n#install.packages(tidyVerse)\n\n\n\n\n\nThis example dataset sourced from kaggle was obtained from surveys of students in Math and Portuguese classes in secondary school. It contains demographic information on gender, social and study information.\n\n\nCode\n# load the dataset\nstu_math &lt;- read.csv(\"student-mat.csv\")\n\n\n\n\nCode\n# load relevant libraries\nlibrary(rcompanion)\nlibrary(car)\nlibrary (gt)\nlibrary(gtsummary)\nlibrary(ggpubr)\nlibrary(rstatix)\nlibrary(tidyverse)\n\n\nChecking the data\n\n\nCode\n# check the data\nglimpse(stu_math)\n\n\nRows: 395\nColumns: 33\n$ school     &lt;chr&gt; \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\",…\n$ sex        &lt;chr&gt; \"F\", \"F\", \"F\", \"F\", \"F\", \"M\", \"M\", \"F\", \"M\", \"M\", \"F\", \"F\",…\n$ age        &lt;int&gt; 18, 17, 15, 15, 16, 16, 16, 17, 15, 15, 15, 15, 15, 15, 15,…\n$ address    &lt;chr&gt; \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\",…\n$ famsize    &lt;chr&gt; \"GT3\", \"GT3\", \"LE3\", \"GT3\", \"GT3\", \"LE3\", \"LE3\", \"GT3\", \"LE…\n$ Pstatus    &lt;chr&gt; \"A\", \"T\", \"T\", \"T\", \"T\", \"T\", \"T\", \"A\", \"A\", \"T\", \"T\", \"T\",…\n$ Medu       &lt;int&gt; 4, 1, 1, 4, 3, 4, 2, 4, 3, 3, 4, 2, 4, 4, 2, 4, 4, 3, 3, 4,…\n$ Fedu       &lt;int&gt; 4, 1, 1, 2, 3, 3, 2, 4, 2, 4, 4, 1, 4, 3, 2, 4, 4, 3, 2, 3,…\n$ Mjob       &lt;chr&gt; \"at_home\", \"at_home\", \"at_home\", \"health\", \"other\", \"servic…\n$ Fjob       &lt;chr&gt; \"teacher\", \"other\", \"other\", \"services\", \"other\", \"other\", …\n$ reason     &lt;chr&gt; \"course\", \"course\", \"other\", \"home\", \"home\", \"reputation\", …\n$ guardian   &lt;chr&gt; \"mother\", \"father\", \"mother\", \"mother\", \"father\", \"mother\",…\n$ traveltime &lt;int&gt; 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 2, 1, 1, 1, 3, 1, 1,…\n$ studytime  &lt;int&gt; 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 1, 2, 3, 1, 3, 2, 1, 1,…\n$ failures   &lt;int&gt; 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0,…\n$ schoolsup  &lt;chr&gt; \"yes\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"yes\", \"no\", \"n…\n$ famsup     &lt;chr&gt; \"no\", \"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", \"yes\",…\n$ paid       &lt;chr&gt; \"no\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\", \"no\", \"no\", \"yes\", …\n$ activities &lt;chr&gt; \"no\", \"no\", \"no\", \"yes\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"ye…\n$ nursery    &lt;chr&gt; \"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes…\n$ higher     &lt;chr&gt; \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"ye…\n$ internet   &lt;chr&gt; \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", \"yes\", \"no\", \"yes\",…\n$ romantic   &lt;chr&gt; \"no\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\"…\n$ famrel     &lt;int&gt; 4, 5, 4, 3, 4, 5, 4, 4, 4, 5, 3, 5, 4, 5, 4, 4, 3, 5, 5, 3,…\n$ freetime   &lt;int&gt; 3, 3, 3, 2, 3, 4, 4, 1, 2, 5, 3, 2, 3, 4, 5, 4, 2, 3, 5, 1,…\n$ goout      &lt;int&gt; 4, 3, 2, 2, 2, 2, 4, 4, 2, 1, 3, 2, 3, 3, 2, 4, 3, 2, 5, 3,…\n$ Dalc       &lt;int&gt; 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1,…\n$ Walc       &lt;int&gt; 1, 1, 3, 1, 2, 2, 1, 1, 1, 1, 2, 1, 3, 2, 1, 2, 2, 1, 4, 3,…\n$ health     &lt;int&gt; 3, 3, 3, 5, 5, 5, 3, 1, 1, 5, 2, 4, 5, 3, 3, 2, 2, 4, 5, 5,…\n$ absences   &lt;int&gt; 6, 4, 10, 2, 4, 10, 0, 6, 0, 0, 0, 4, 2, 2, 0, 4, 6, 4, 16,…\n$ G1         &lt;int&gt; 5, 5, 7, 15, 6, 15, 12, 6, 16, 14, 10, 10, 14, 10, 14, 14, …\n$ G2         &lt;int&gt; 6, 5, 8, 14, 10, 15, 12, 5, 18, 15, 8, 12, 14, 10, 16, 14, …\n$ G3         &lt;int&gt; 6, 6, 10, 15, 10, 15, 11, 6, 19, 15, 9, 12, 14, 11, 16, 14,…\n\n\nIn total there are 395 observations and 33 variables. We will drop the variables we do not need and keep the variables that will help us answer the following: Is there a difference between boys and girls in math final grades?\nH_0: There is no statistical difference between the final grades between boys and girls.\nH_1: There is a statistically significant difference in the final grades between the two groups.\n\n\nCode\n# creating a subset of the data \nmath = subset(stu_math, select= c(sex,G3))\nglimpse(math)\n\n\nRows: 395\nColumns: 2\n$ sex &lt;chr&gt; \"F\", \"F\", \"F\", \"F\", \"F\", \"M\", \"M\", \"F\", \"M\", \"M\", \"F\", \"F\", \"M\", \"…\n$ G3  &lt;int&gt; 6, 6, 10, 15, 10, 15, 11, 6, 19, 15, 9, 12, 14, 11, 16, 14, 14, 10…\n\n\nSummary statistics- the dependent variable is continuous (grades=G3) and the independent variable is character but binary (sex).\n\n\nCode\n# summarizing our data\n summary(math)\n\n\n     sex                  G3       \n Length:395         Min.   : 0.00  \n Class :character   1st Qu.: 8.00  \n Mode  :character   Median :11.00  \n                    Mean   :10.42  \n                    3rd Qu.:14.00  \n                    Max.   :20.00  \n\n\nWe see that data ranges from 0-20 with 0 being people who were absent and could not take the test therefore missing data. We remove these 0 values before running the t test. However other models should be considered such as the zero inflated model to differentiate those who truly got a 0 and those who were not present to take test.\n\n\nCode\n# creating a boxplot to visualize the data with no outliers\nmath2 = subset(math, G3&gt;0)\nboxplot(G3 ~ sex,data=math2)\n\n\n\n\n\nVisualizing the data- we can use histograms and box lots to visualize the data to check for outliers and distribution thus checking for normality.\n\n\nCode\n# Histograms for data by groups \n\nmale = math2$G3[math2$sex == \"M\"]\nfemale = math2$G3[math2$sex == \"F\"]\n\n# plotting distribution for males\nplotNormalHistogram(male, breaks= 20,xlim=c(0,20),main=\"Distribution of the grades for males \", xlab= \"Math Grades\")\n\n\n\n\n\nFinal grades for males seem to be normally distributed from 0-20. Data is approximately normal because we have a large amount of bins.\n\n\nCode\n# plotting distribution for females\nplotNormalHistogram(female, breaks= 20,xlim=c(0,20),main=\"Distribution of the grades for females \", xlab= \"Math Grades\")\n\n\n\n\n\nFinal grades for females also appear to be normally distributed. The final score across both is almost evenly distributed. However there seem to be a significant number of individuals who failed the test (grade=0).\n\n\nCode\n# plotting bar plot to see the distribution in sample size\nsample_size = table(math2$sex)\nbarplot(sample_size,main= \"Distribution of sample size by sex\")\n\n\n\n\n\nThe bar graph shows that there are slightly more females in the sample than males.\nIdentifying outliers\n\n\nCode\n# creating a boxplot to visualize the outliers (G3=0)\nboxplot(G3 ~ sex,data=math2)\n\n\n\n\n\nThe box plot shows us that there are no outliers as these have been removed in terms of people who had a score of 0. This score is not truly reflective of the performance between boys and girls as a grade of 0 may represent absentia or other reasons for the test not been taken. Therefore we opt to drop the outliers. We will compare to see if this decision affects the mean which appears similar from the above plot.\n\n\nCode\n# finding the mean for the groups with outliers\nmean(math$G3[math$sex==\"F\"])\n\n\n[1] 9.966346\n\n\nCode\nmean(math$G3[math$sex==\"M\"])\n\n\n[1] 10.91444\n\n\nCode\n# finding the mean for the groups without outliers\nmean(math2$G3[math2$sex==\"F\"])\n\n\n[1] 11.20541\n\n\nCode\nmean(math2$G3[math2$sex==\"M\"])\n\n\n[1] 11.86628\n\n\nThe mean has increased slightly and the difference decreased after removing the outliers but the distribution is still the same.\nCheck the equality of variances (homogeneity)\nWe can use the Levene’s test or the Bartlett’s test to check for homogeneity of variances. The former is in the car library and the later in the rstatix library. If the variances are homogeneous the p value will be greater than 0.05.\nOther tests include F test 2 sided, Brown-Forsythe and O’Brien but we shall not cover these.\n\n\nCode\n# running the Bartleet's test to check equal variance\nbartlett.test(G3~sex, data=math2)\n\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  G3 by sex\nBartlett's K-squared = 0.12148, df = 1, p-value = 0.7274\n\n\nCode\n# running the Levene's test to check equal variance\nmath2 %&gt;% levene_test(G3~sex)\n\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     1   355     0.614 0.434\n\n\nThe p value is greater than 0.05 suggesting there is no difference between the variances of the two groups.\n\n\n\n\nData is continuous(G3)\nData is normally distributed\nData is independent (males and females distinct not the same individual)\nNo significant outliers\nThere are equal variances\n\nAs the assumptions are met we go ahead to perform the Student’s t test.\n\n\n\nSince the default is the Welch t test we use the \\color{blue}{\\text{var.eqaul = TRUE }} code to signify a Student’s t test. There is a t.test() function in stats package and a t_test() in the rstatix package. For this analysis we use the rstatix method as it comes out as a table.\n\n\nCode\n# perfoming the two sample t test\nstat.test &lt;- math2 %&gt;% \n  t_test(G3~ sex, var.equal=TRUE) %&gt;%\n  add_significance()\nstat.test\n\n\n# A tibble: 1 × 9\n  .y.   group1 group2    n1    n2 statistic    df      p p.signif\n  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   \n1 G3    F      M        185   172     -1.94   355 0.0531 ns      \n\n\n\n\nCode\nstat.test$statistic\n\n\n        t \n-1.940477 \n\n\nThe results are represented as follows;\n\ny - dependent variable\ngroup1, group 2 - compared groups(independent variables)\ndf - degrees of freedom\np - p value\n\ngtsummary table of results\n\n\nCode\n math2 |&gt; \n  tbl_summary(\n    by = sex,\n    statistic =\n      list(\n        all_continuous() ~ \"{mean} ({sd})\",\n        all_dichotomous() ~ \"{p}%\")\n    ) |&gt; \n   add_n() |&gt; \n  add_overall() |&gt; \n  add_difference()\n\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      N\n      Overall, N = 3571\n      F, N = 1851\n      M, N = 1721\n      Difference2\n      95% CI2,3\n      p-value2\n    \n  \n  \n    G3\n357\n12 (3)\n11 (3)\n12 (3)\n-0.66\n-1.3, 0.01\n0.053\n  \n  \n  \n    \n      1 Mean (SD)\n    \n    \n      2 Welch Two Sample t-test\n    \n    \n      3 CI = Confidence Interval\n    \n  \n\n\n\n\nInterpretation of results\nFor the two sample t test with t(355) = -1.940477, p &lt; 0.0531, the p value is greater than our alpha of 0.05 , we fail to reject the null hypothesis and conclude that there is no statistical difference between the means of the two groups. There is no difference in final grades between boys and girls. (A significant |t| would be 1.96).\nEffect size\nCohen’s d can be an used as an effect size statistic for the two sample t test. It is the difference between the means of each group divided by the pooled standard deviation.\nd= {m_A-m_B \\over SD_pooled}\nIt ranges from 0 to infinity, with 0 indicating no effect where the means are equal. 0.5 means that the means differ by half the standard deviation of the data and 1 means they differ by 1 standard deviation. It is divided into small, medium or large using the following cut off points.\n\nsmall 0.2-&lt;0.5\nmedium 0.5-&lt;0.8\nlarge &gt;=0.8\n\nFor the above test the following is how we can find the effect size;\n\n\nCode\n#perfoming cohen's d\nmath2 %&gt;% \n  cohens_d(G3~sex,var.equal = TRUE)\n\n\n# A tibble: 1 × 7\n  .y.   group1 group2 effsize    n1    n2 magnitude\n* &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;ord&gt;    \n1 G3    F      M       -0.206   185   172 small    \n\n\nThe effect size is small d= -0.20.\nIn conclusion, a two-samples t-test showed that the difference was not statistically significant, t(355) = -1.940477, p &lt; 0.0531, d = -0.20; where, t(355) is shorthand notation for a t-statistic that has 355 degrees of freedom and d is Cohen’s d. We can conclude that the females mean final grade is greater than males final grade (d= -0.20) but this result is not significant.\nWhat if it is one tailed t test?\nUse the \\color{blue}{\\text{alternative =}} option to determine if one group is \\color{blue}{\\text{\"less\"}} or \\color{blue}{\\text{\"greater\"}}. For example if we want to see whether the final grades for females are greater than males we can use the following code:\n\n\nCode\n# perfoming the one tailed two sample t test\nstat.test &lt;- math2 %&gt;% \n  t_test(G3~ sex, var.equal=TRUE, alternative = \"greater\") %&gt;%\n  add_significance()\nstat.test\n\n\n# A tibble: 1 × 9\n  .y.   group1 group2    n1    n2 statistic    df     p p.signif\n  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   \n1 G3    F      M        185   172     -1.94   355 0.973 ns      \n\n\nThe p value is greater than 0.05 (p=0.973), we fail to reject the null hypothesis. We conclude that the final grades for females are not significantly greater than for males.\nWhat about running the paired sample t test?\nWe can simply add the syntax \\color{blue}{\\text{paired= TRUE}} to our t_test() to run the analysis for matched pairs data.\n\n\n\n\nThis article covers the Student’s t test and how we run it in R. It also shows how we find the effect size and how we can conclude the results."
  },
  {
    "objectID": "Two_sample_t.html#assumptions",
    "href": "Two_sample_t.html#assumptions",
    "title": "Two sample t test",
    "section": "",
    "text": "Measurements for one observation do not affect measurements for any other observation (assumes independence).\nData in each group must be obtained via a random sample from the population.\nData in each group are normally distributed.\nData values are continuous.\nThe variances for the two independent groups are equal in the Student’s t test.\nThere should be no significant outliers."
  },
  {
    "objectID": "Two_sample_t.html#hypotheses",
    "href": "Two_sample_t.html#hypotheses",
    "title": "Two sample t test",
    "section": "",
    "text": "(H_0): the mean of group A (m_A) is equal to the mean of group B (m_B)- two tailed test.\n(H_0): the mean of group A (m_A) is greater than or equal to mean of group B (m_B)- one tailed test.\n(H_0): the mean of group A (m_A) is less than or equal to the mean of group B (m_B)- one tailed test.\nThe corresponding alternative hypotheses would be as follows:\n\n\n\n(H_1): the mean of group A (m_A) is different to the mean of group B (m_B)- two tailed test.\n(H_1): the mean of group A (m_A) is less than the mean of group B (m_B)- one tailed test.\n(H_1): the mean of group A (m_A) is greater the mean of group B (m_B)- one tailed test."
  },
  {
    "objectID": "Two_sample_t.html#statistical-hypotheses-formula",
    "href": "Two_sample_t.html#statistical-hypotheses-formula",
    "title": "Two sample t test",
    "section": "",
    "text": "For the Student’s t test which assumes equal variance the following is how the |t| statistic may be calculated using groups A and B as examples:\nt ={ {\\bar{x}_{1} - \\bar{x}_{2}} \\over \\sqrt{ s^2( {1 \\over n_1 } + {1 \\over n_2}) }}\nThis can be described as the sample mean difference divided by the sample standard deviation of the sample mean difference where:\nm_A and m_B are the mean values of A and B,\nn_A and n_B are the seize of group A and B,\nS^2 is the estimator for the pooled variance,\nwith the degrees of freedom (df) = n_A + n_B - 2,\nand s^2 is calculated as follows:\ns^2 = { {\\displaystyle\\sum_{i=1}^{n_1} { (x_i-\\bar{x}_1)^2} + \\displaystyle\\sum_{j=1}^{n_2}{ (x_j-\\bar{x}_2)^2}} \\over {n_{1} + n_{2} - 2 }}\nResults for both Students t test and Welch’s t test are usually similar unless the group sizes and standard deviations are different.\nWhat if the data is not independent?\nIf the data is not independent such as paired data in the form of matched pairs which are correlated, we use the paired t test. This test checks whether the means of two paired groups are different from each other. It’s usually used in clinical trial studies with a “before and after” or case control studies with matched pairs. For this test we only assume the difference of each pair to be normally distributed (the paired groups are the ones important for analysis) unlike the independent t test which assumes that data from both samples are independent and variances are equal.[@fralick]"
  },
  {
    "objectID": "Two_sample_t.html#example",
    "href": "Two_sample_t.html#example",
    "title": "Two sample t test",
    "section": "",
    "text": "tidyverse: data manipulation and visualization.\nrstatix: providing pipe friendly R functions for easy statistical analyses.\ncar: providing variance tests.\n\n\n\nCode\n#install.packages(\"ggstatplot\") \n#install.packages(\"car\")\n#install.packages(\"rstatix\")\n#install.packages(tidyVerse)\n\n\n\n\n\nThis example dataset sourced from kaggle was obtained from surveys of students in Math and Portuguese classes in secondary school. It contains demographic information on gender, social and study information.\n\n\nCode\n# load the dataset\nstu_math &lt;- read.csv(\"student-mat.csv\")\n\n\n\n\nCode\n# load relevant libraries\nlibrary(rcompanion)\nlibrary(car)\nlibrary (gt)\nlibrary(gtsummary)\nlibrary(ggpubr)\nlibrary(rstatix)\nlibrary(tidyverse)\n\n\nChecking the data\n\n\nCode\n# check the data\nglimpse(stu_math)\n\n\nRows: 395\nColumns: 33\n$ school     &lt;chr&gt; \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\", \"GP\",…\n$ sex        &lt;chr&gt; \"F\", \"F\", \"F\", \"F\", \"F\", \"M\", \"M\", \"F\", \"M\", \"M\", \"F\", \"F\",…\n$ age        &lt;int&gt; 18, 17, 15, 15, 16, 16, 16, 17, 15, 15, 15, 15, 15, 15, 15,…\n$ address    &lt;chr&gt; \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\", \"U\",…\n$ famsize    &lt;chr&gt; \"GT3\", \"GT3\", \"LE3\", \"GT3\", \"GT3\", \"LE3\", \"LE3\", \"GT3\", \"LE…\n$ Pstatus    &lt;chr&gt; \"A\", \"T\", \"T\", \"T\", \"T\", \"T\", \"T\", \"A\", \"A\", \"T\", \"T\", \"T\",…\n$ Medu       &lt;int&gt; 4, 1, 1, 4, 3, 4, 2, 4, 3, 3, 4, 2, 4, 4, 2, 4, 4, 3, 3, 4,…\n$ Fedu       &lt;int&gt; 4, 1, 1, 2, 3, 3, 2, 4, 2, 4, 4, 1, 4, 3, 2, 4, 4, 3, 2, 3,…\n$ Mjob       &lt;chr&gt; \"at_home\", \"at_home\", \"at_home\", \"health\", \"other\", \"servic…\n$ Fjob       &lt;chr&gt; \"teacher\", \"other\", \"other\", \"services\", \"other\", \"other\", …\n$ reason     &lt;chr&gt; \"course\", \"course\", \"other\", \"home\", \"home\", \"reputation\", …\n$ guardian   &lt;chr&gt; \"mother\", \"father\", \"mother\", \"mother\", \"father\", \"mother\",…\n$ traveltime &lt;int&gt; 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 2, 1, 1, 1, 3, 1, 1,…\n$ studytime  &lt;int&gt; 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 1, 2, 3, 1, 3, 2, 1, 1,…\n$ failures   &lt;int&gt; 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0,…\n$ schoolsup  &lt;chr&gt; \"yes\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"yes\", \"no\", \"n…\n$ famsup     &lt;chr&gt; \"no\", \"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", \"yes\",…\n$ paid       &lt;chr&gt; \"no\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\", \"no\", \"no\", \"yes\", …\n$ activities &lt;chr&gt; \"no\", \"no\", \"no\", \"yes\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"ye…\n$ nursery    &lt;chr&gt; \"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes…\n$ higher     &lt;chr&gt; \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"ye…\n$ internet   &lt;chr&gt; \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", \"yes\", \"no\", \"yes\",…\n$ romantic   &lt;chr&gt; \"no\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\"…\n$ famrel     &lt;int&gt; 4, 5, 4, 3, 4, 5, 4, 4, 4, 5, 3, 5, 4, 5, 4, 4, 3, 5, 5, 3,…\n$ freetime   &lt;int&gt; 3, 3, 3, 2, 3, 4, 4, 1, 2, 5, 3, 2, 3, 4, 5, 4, 2, 3, 5, 1,…\n$ goout      &lt;int&gt; 4, 3, 2, 2, 2, 2, 4, 4, 2, 1, 3, 2, 3, 3, 2, 4, 3, 2, 5, 3,…\n$ Dalc       &lt;int&gt; 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1,…\n$ Walc       &lt;int&gt; 1, 1, 3, 1, 2, 2, 1, 1, 1, 1, 2, 1, 3, 2, 1, 2, 2, 1, 4, 3,…\n$ health     &lt;int&gt; 3, 3, 3, 5, 5, 5, 3, 1, 1, 5, 2, 4, 5, 3, 3, 2, 2, 4, 5, 5,…\n$ absences   &lt;int&gt; 6, 4, 10, 2, 4, 10, 0, 6, 0, 0, 0, 4, 2, 2, 0, 4, 6, 4, 16,…\n$ G1         &lt;int&gt; 5, 5, 7, 15, 6, 15, 12, 6, 16, 14, 10, 10, 14, 10, 14, 14, …\n$ G2         &lt;int&gt; 6, 5, 8, 14, 10, 15, 12, 5, 18, 15, 8, 12, 14, 10, 16, 14, …\n$ G3         &lt;int&gt; 6, 6, 10, 15, 10, 15, 11, 6, 19, 15, 9, 12, 14, 11, 16, 14,…\n\n\nIn total there are 395 observations and 33 variables. We will drop the variables we do not need and keep the variables that will help us answer the following: Is there a difference between boys and girls in math final grades?\nH_0: There is no statistical difference between the final grades between boys and girls.\nH_1: There is a statistically significant difference in the final grades between the two groups.\n\n\nCode\n# creating a subset of the data \nmath = subset(stu_math, select= c(sex,G3))\nglimpse(math)\n\n\nRows: 395\nColumns: 2\n$ sex &lt;chr&gt; \"F\", \"F\", \"F\", \"F\", \"F\", \"M\", \"M\", \"F\", \"M\", \"M\", \"F\", \"F\", \"M\", \"…\n$ G3  &lt;int&gt; 6, 6, 10, 15, 10, 15, 11, 6, 19, 15, 9, 12, 14, 11, 16, 14, 14, 10…\n\n\nSummary statistics- the dependent variable is continuous (grades=G3) and the independent variable is character but binary (sex).\n\n\nCode\n# summarizing our data\n summary(math)\n\n\n     sex                  G3       \n Length:395         Min.   : 0.00  \n Class :character   1st Qu.: 8.00  \n Mode  :character   Median :11.00  \n                    Mean   :10.42  \n                    3rd Qu.:14.00  \n                    Max.   :20.00  \n\n\nWe see that data ranges from 0-20 with 0 being people who were absent and could not take the test therefore missing data. We remove these 0 values before running the t test. However other models should be considered such as the zero inflated model to differentiate those who truly got a 0 and those who were not present to take test.\n\n\nCode\n# creating a boxplot to visualize the data with no outliers\nmath2 = subset(math, G3&gt;0)\nboxplot(G3 ~ sex,data=math2)\n\n\n\n\n\nVisualizing the data- we can use histograms and box lots to visualize the data to check for outliers and distribution thus checking for normality.\n\n\nCode\n# Histograms for data by groups \n\nmale = math2$G3[math2$sex == \"M\"]\nfemale = math2$G3[math2$sex == \"F\"]\n\n# plotting distribution for males\nplotNormalHistogram(male, breaks= 20,xlim=c(0,20),main=\"Distribution of the grades for males \", xlab= \"Math Grades\")\n\n\n\n\n\nFinal grades for males seem to be normally distributed from 0-20. Data is approximately normal because we have a large amount of bins.\n\n\nCode\n# plotting distribution for females\nplotNormalHistogram(female, breaks= 20,xlim=c(0,20),main=\"Distribution of the grades for females \", xlab= \"Math Grades\")\n\n\n\n\n\nFinal grades for females also appear to be normally distributed. The final score across both is almost evenly distributed. However there seem to be a significant number of individuals who failed the test (grade=0).\n\n\nCode\n# plotting bar plot to see the distribution in sample size\nsample_size = table(math2$sex)\nbarplot(sample_size,main= \"Distribution of sample size by sex\")\n\n\n\n\n\nThe bar graph shows that there are slightly more females in the sample than males.\nIdentifying outliers\n\n\nCode\n# creating a boxplot to visualize the outliers (G3=0)\nboxplot(G3 ~ sex,data=math2)\n\n\n\n\n\nThe box plot shows us that there are no outliers as these have been removed in terms of people who had a score of 0. This score is not truly reflective of the performance between boys and girls as a grade of 0 may represent absentia or other reasons for the test not been taken. Therefore we opt to drop the outliers. We will compare to see if this decision affects the mean which appears similar from the above plot.\n\n\nCode\n# finding the mean for the groups with outliers\nmean(math$G3[math$sex==\"F\"])\n\n\n[1] 9.966346\n\n\nCode\nmean(math$G3[math$sex==\"M\"])\n\n\n[1] 10.91444\n\n\nCode\n# finding the mean for the groups without outliers\nmean(math2$G3[math2$sex==\"F\"])\n\n\n[1] 11.20541\n\n\nCode\nmean(math2$G3[math2$sex==\"M\"])\n\n\n[1] 11.86628\n\n\nThe mean has increased slightly and the difference decreased after removing the outliers but the distribution is still the same.\nCheck the equality of variances (homogeneity)\nWe can use the Levene’s test or the Bartlett’s test to check for homogeneity of variances. The former is in the car library and the later in the rstatix library. If the variances are homogeneous the p value will be greater than 0.05.\nOther tests include F test 2 sided, Brown-Forsythe and O’Brien but we shall not cover these.\n\n\nCode\n# running the Bartleet's test to check equal variance\nbartlett.test(G3~sex, data=math2)\n\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  G3 by sex\nBartlett's K-squared = 0.12148, df = 1, p-value = 0.7274\n\n\nCode\n# running the Levene's test to check equal variance\nmath2 %&gt;% levene_test(G3~sex)\n\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     1   355     0.614 0.434\n\n\nThe p value is greater than 0.05 suggesting there is no difference between the variances of the two groups.\n\n\n\n\nData is continuous(G3)\nData is normally distributed\nData is independent (males and females distinct not the same individual)\nNo significant outliers\nThere are equal variances\n\nAs the assumptions are met we go ahead to perform the Student’s t test.\n\n\n\nSince the default is the Welch t test we use the \\color{blue}{\\text{var.eqaul = TRUE }} code to signify a Student’s t test. There is a t.test() function in stats package and a t_test() in the rstatix package. For this analysis we use the rstatix method as it comes out as a table.\n\n\nCode\n# perfoming the two sample t test\nstat.test &lt;- math2 %&gt;% \n  t_test(G3~ sex, var.equal=TRUE) %&gt;%\n  add_significance()\nstat.test\n\n\n# A tibble: 1 × 9\n  .y.   group1 group2    n1    n2 statistic    df      p p.signif\n  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   \n1 G3    F      M        185   172     -1.94   355 0.0531 ns      \n\n\n\n\nCode\nstat.test$statistic\n\n\n        t \n-1.940477 \n\n\nThe results are represented as follows;\n\ny - dependent variable\ngroup1, group 2 - compared groups(independent variables)\ndf - degrees of freedom\np - p value\n\ngtsummary table of results\n\n\nCode\n math2 |&gt; \n  tbl_summary(\n    by = sex,\n    statistic =\n      list(\n        all_continuous() ~ \"{mean} ({sd})\",\n        all_dichotomous() ~ \"{p}%\")\n    ) |&gt; \n   add_n() |&gt; \n  add_overall() |&gt; \n  add_difference()\n\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      N\n      Overall, N = 3571\n      F, N = 1851\n      M, N = 1721\n      Difference2\n      95% CI2,3\n      p-value2\n    \n  \n  \n    G3\n357\n12 (3)\n11 (3)\n12 (3)\n-0.66\n-1.3, 0.01\n0.053\n  \n  \n  \n    \n      1 Mean (SD)\n    \n    \n      2 Welch Two Sample t-test\n    \n    \n      3 CI = Confidence Interval\n    \n  \n\n\n\n\nInterpretation of results\nFor the two sample t test with t(355) = -1.940477, p &lt; 0.0531, the p value is greater than our alpha of 0.05 , we fail to reject the null hypothesis and conclude that there is no statistical difference between the means of the two groups. There is no difference in final grades between boys and girls. (A significant |t| would be 1.96).\nEffect size\nCohen’s d can be an used as an effect size statistic for the two sample t test. It is the difference between the means of each group divided by the pooled standard deviation.\nd= {m_A-m_B \\over SD_pooled}\nIt ranges from 0 to infinity, with 0 indicating no effect where the means are equal. 0.5 means that the means differ by half the standard deviation of the data and 1 means they differ by 1 standard deviation. It is divided into small, medium or large using the following cut off points.\n\nsmall 0.2-&lt;0.5\nmedium 0.5-&lt;0.8\nlarge &gt;=0.8\n\nFor the above test the following is how we can find the effect size;\n\n\nCode\n#perfoming cohen's d\nmath2 %&gt;% \n  cohens_d(G3~sex,var.equal = TRUE)\n\n\n# A tibble: 1 × 7\n  .y.   group1 group2 effsize    n1    n2 magnitude\n* &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;ord&gt;    \n1 G3    F      M       -0.206   185   172 small    \n\n\nThe effect size is small d= -0.20.\nIn conclusion, a two-samples t-test showed that the difference was not statistically significant, t(355) = -1.940477, p &lt; 0.0531, d = -0.20; where, t(355) is shorthand notation for a t-statistic that has 355 degrees of freedom and d is Cohen’s d. We can conclude that the females mean final grade is greater than males final grade (d= -0.20) but this result is not significant.\nWhat if it is one tailed t test?\nUse the \\color{blue}{\\text{alternative =}} option to determine if one group is \\color{blue}{\\text{\"less\"}} or \\color{blue}{\\text{\"greater\"}}. For example if we want to see whether the final grades for females are greater than males we can use the following code:\n\n\nCode\n# perfoming the one tailed two sample t test\nstat.test &lt;- math2 %&gt;% \n  t_test(G3~ sex, var.equal=TRUE, alternative = \"greater\") %&gt;%\n  add_significance()\nstat.test\n\n\n# A tibble: 1 × 9\n  .y.   group1 group2    n1    n2 statistic    df     p p.signif\n  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   \n1 G3    F      M        185   172     -1.94   355 0.973 ns      \n\n\nThe p value is greater than 0.05 (p=0.973), we fail to reject the null hypothesis. We conclude that the final grades for females are not significantly greater than for males.\nWhat about running the paired sample t test?\nWe can simply add the syntax \\color{blue}{\\text{paired= TRUE}} to our t_test() to run the analysis for matched pairs data."
  },
  {
    "objectID": "Two_sample_t.html#conclusion",
    "href": "Two_sample_t.html#conclusion",
    "title": "Two sample t test",
    "section": "",
    "text": "This article covers the Student’s t test and how we run it in R. It also shows how we find the effect size and how we can conclude the results."
  },
  {
    "objectID": "Fishersexactest.html",
    "href": "Fishersexactest.html",
    "title": "Fisher’s Exact Test",
    "section": "",
    "text": "Fisher’s exact test is an independent test used to determine if there is a relationship between categorical (non-parametric) variables with a small sample size.\nUsed to assess whether proportions of one variable are different among values of another table.\nUses (hypergeometric) marginal distribution to derive exact p-values which are not approximated which are somewhat conservative.\nThe rules of Chi distribution do not apply when the frequency count is &lt;5 for more than 20% of the cells in a contingency table (Bower 2003).\nData is easily manipulated by using a contingency table.\n\n\n\n\nAssumes that the individual observations are independent.\nAssumes that the row and column totals are fixed or conditioned.\nThe variables are categorical and randomly sampled.\nObservations are count data.\n\n\n\n\nThe hypotheses of Fisher’s exact test are similar to Chi-square test:\nNull hypothesis:(H_0) There is no relationship between the categorical variables, the variables are independent.\nAlternative hypothesis: (H_1) There is a relationship between the categorical variables, the variables are dependent."
  },
  {
    "objectID": "Fishersexactest.html#introduction",
    "href": "Fishersexactest.html#introduction",
    "title": "Fisher’s Exact Test",
    "section": "",
    "text": "Fisher’s exact test is an independent test used to determine if there is a relationship between categorical (non-parametric) variables with a small sample size.\nUsed to assess whether proportions of one variable are different among values of another table.\nUses (hypergeometric) marginal distribution to derive exact p-values which are not approximated which are somewhat conservative.\nThe rules of Chi distribution do not apply when the frequency count is &lt;5 for more than 20% of the cells in a contingency table (Bower 2003).\nData is easily manipulated by using a contingency table.\n\n\n\n\nAssumes that the individual observations are independent.\nAssumes that the row and column totals are fixed or conditioned.\nThe variables are categorical and randomly sampled.\nObservations are count data.\n\n\n\n\nThe hypotheses of Fisher’s exact test are similar to Chi-square test:\nNull hypothesis:(H_0) There is no relationship between the categorical variables, the variables are independent.\nAlternative hypothesis: (H_1) There is a relationship between the categorical variables, the variables are dependent."
  },
  {
    "objectID": "Fishersexactest.html#fishers-exact-test-equation",
    "href": "Fishersexactest.html#fishers-exact-test-equation",
    "title": "Fisher’s Exact Test",
    "section": "Fisher’s Exact Test Equation",
    "text": "Fisher’s Exact Test Equation\nFisher’s exact test for a one-tailed p-value is calculated using the following formula:\n   p = {(a+b)!(c+d)!(a+c)!(b+d)! \\over a! b! c! d! n!}  - n = population size/ total frequency - a + b = “successes” values in the contingency table - a + c = sample size / draws from the population - a = sample successes\n\nFormula description\nthis test is usually used as a one-tailed test but it can also be used as a two tailed test as well, a,b,c, and d are the individual frequencies on the 2x2 contingency table and n is our total frequency. This particular test is used to obtain the probability of the combination of frequencies that we can actually obtain.\n\n\nWhat is a contingency table?\nThis is a table that shows the distribution of a variable in the rows and columns. Sometimes referred to as a 2x2 table. They are useful in summarizing categorical variables. The table() function is used to create a contingency table in R. When the variables of interest are summarized in a contingency table it is easier to run the Fisher’s Exact test.\n\nExample: Creating a contingency table\nLets say we have information on the gender of participants in a clinical trial and the type of drug administered to them we can create the following contingency table for further analysis.\n\n\nCode\n# Example R code to create a contingency table\n\n# Creating a data frame\n df = data.frame (\n   \"Drug\" = c(\"Drug A\", \"Drug B\", \"Drug A\"),\n   \"Gender\" = c(\"Male\", \"Male\", \"Female\")\n )\n \n# Creating contingency table using table()\n ctable = table(df)\n print(ctable)\n\n\n        Gender\nDrug     Female Male\n  Drug A      1    1\n  Drug B      0    1\n\n\n\n\n\nPerforming Fisher’s Exact Test in R\nWe will need to install the ggstatplot package to visualize the statistical results.\n\n\nCode\n#install.packages(\"ggstatplot\") \n#install.packages(\"summarytools\")\n#install.packages(\"gmodels\")\n#install.packages(\"gt\")\n#install.packages(\"tidyverse\")\n\n\n\n\nData Source: GMP2017\nFor this example we will be using the Greater Manchester Police’s UK stop and search data from 2017(December) sourced from the Sage Research Methods Dataset Part 2. This data has information on stop and search events, gender and ethnicity. For this example we would like to access whether there is a significant relationship between gender and stop and search events?\n\n\nCode\nGMP17 &lt;- read.csv(\"dataset-gmss-2017-subset1.csv\")\n\n\n\n\nLoad in libraries\n\n\nCode\nlibrary(gmodels)\nlibrary(ggstatsplot)\nlibrary(gt)\nlibrary(gtsummary)\nlibrary(katex)\nlibrary(tidyverse)\n\n\n\n\nDescriptive summary\n\nhead(GMP17)\n\n  Gender Ethnicity ObjectSearch\n1      1         1            1\n2      1         1           -9\n3      1         1            1\n4      1         1            1\n5      1         1           -9\n6      1         1            1\n\nstr(GMP17)\n\n'data.frame':   186 obs. of  3 variables:\n $ Gender      : int  1 1 1 1 1 1 1 1 1 -9 ...\n $ Ethnicity   : int  1 1 1 1 1 1 2 1 1 1 ...\n $ ObjectSearch: int  1 -9 1 1 -9 1 1 1 -9 -9 ...\n\n# determining the number of rows\nNROW(GMP17)\n\n[1] 186\n\n\n\n\nAssessing frequencies to answer research question\nFor this analysis we will use the Gender variable and the ObjectSearch variable\n\n\nCode\n# Dropping the Ethnicity variable to remain with variables of interest for for the 2x2 table\n\nnewGMP17 &lt;-GMP17[ -c(2) ]\n \nhead(newGMP17)\n\n\n  Gender ObjectSearch\n1      1            1\n2      1           -9\n3      1            1\n4      1            1\n5      1           -9\n6      1            1\n\n\nThe data contains missing values categorized as -9 that we need to drop and we need to rename our variables based on the data dictionary provided https://methods.sagepub.com/dataset/fishers-exact-gmss-2017-r.\n\n\nCode\n# Exclude rows that have missing data in both variables\nnewGMP17_nom &lt;- subset(newGMP17, Gender &gt; 0)\nnewGMP17_nom2 &lt;- subset(newGMP17_nom, ObjectSearch  &gt; 0)\nsummary(newGMP17_nom2)\n\n\n     Gender       ObjectSearch  \n Min.   :1.000   Min.   :1.000  \n 1st Qu.:1.000   1st Qu.:1.000  \n Median :1.000   Median :1.000  \n Mean   :1.052   Mean   :1.259  \n 3rd Qu.:1.000   3rd Qu.:2.000  \n Max.   :2.000   Max.   :2.000  \n\n\nCode\nnrow(newGMP17_nom2)\n\n\n[1] 116\n\n\n\n\nCode\n# Renaming the Gender variable based on data dictionary\nnewGMP17_nom2$Gender &lt;- \n  recode_factor(\n    newGMP17_nom2$Gender,\n            \"1\" = \"Male\",\n            \"2\" = \"Female\"\n)\n\n# Renaming the Gender variable based on data dictionary\nnewGMP17_nom2$ObjectSearch &lt;- \n  recode_factor(\n    newGMP17_nom2$ObjectSearch,\n            \"1\" = \"Controlled_Drugs\",\n            \"2\" = \"Harmful_Objects\"\n)\n\n\n\n\nCode\n# Creating the contingency table for subset data\ncGMP17 = table(newGMP17_nom2)\nprint(cGMP17)\n\n\n        ObjectSearch\nGender   Controlled_Drugs Harmful_Objects\n  Male                 84              26\n  Female                2               4\n\n\n\n\nVisualizing data using mosaic plot\n\nwe can use the mosaic plot to represent the data.\n\n\n\nCode\nmosaicplot(cGMP17,\n           main ='Mosaic Plot',\n           color = TRUE)\n\n\n\n\n\n\n\nRunning the Fisher’s exact test using fisher.test()\nWhat if we just run a Chi-square test?\nUsing our GMP17 dataset we can try to run a Chi-square test instead of the Fisher’s Exact test and see what happens.\nThe R output gives us a warning that the Chi Square is not appropriate hence we should use another test in this case the Fisher’s Exact Test.\n\n\nCode\nchisq.test(cGMP17)$expected\n\n\nWarning in chisq.test(cGMP17): Chi-squared approximation may be incorrect\n\n\n        ObjectSearch\nGender   Controlled_Drugs Harmful_Objects\n  Male          81.551724       28.448276\n  Female         4.448276        1.551724\n\n\n\n\nRunning the test\n\n\nCode\n# running the fisher's exact test\n\ntest &lt;- fisher.test(cGMP17)\ntest\n\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  cGMP17\np-value = 0.03809\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  0.8528079 73.5937611\nsample estimates:\nodds ratio \n  6.331977 \n\n\nUsing the gt summary to view results.\n\n\nCode\nnewGMP17_nom2 |&gt; \n  tbl_summary(by = Gender) |&gt; \n   add_p() |&gt; \n  add_overall()\n\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      Overall, N = 1161\n      Male, N = 1101\n      Female, N = 61\n      p-value2\n    \n  \n  \n    ObjectSearch\n\n\n\n0.038\n        Controlled_Drugs\n86 (74%)\n84 (76%)\n2 (33%)\n\n        Harmful_Objects\n30 (26%)\n26 (24%)\n4 (67%)\n\n  \n  \n  \n    \n      1 n (%)\n    \n    \n      2 Fisher’s exact test\n    \n  \n\n\n\n\n\n\nInterpretation of results\nThe most important test statistic is the p - value therefore we can retrieve the specific result using the following code;\n\n\nCode\ntest$p.value \n\n\n[1] 0.03808788\n\n\nOdds ratio = 6.33, 95% CI = 0.85-73.59], we reject the null hypothesis (p &lt; 0.05) and conclude that there is a strong association between the two categorical independent variables (gender and object search events)\nTherefore the odds ratio indicates that the odds of getting stopped and searched by gender is 6.33 times as likely for males compared to females. In other words, males are more likely of getting stopped and searched than females.\n\n\nVisualizing statistical results with plots using ggstatsplot\n\nwe download the ggsattsplot package to visualize the results in a plot.\n\n\n\nCode\n# Fisher's exact test \n\ntest &lt;- fisher.test(cGMP17)\n\n# combine plot and statistical test with ggbarstats\n\nggbarstats(\n newGMP17_nom2, Gender, ObjectSearch,\n results.subtitle = FALSE,\n subtitle = paste0(\n \"Fisher's exact test\", \", p-value = \",\n ifelse(test$p.value &lt; 0.001, \"&lt; 0.001\", round(test$p.value, 3))\n  )\n )\n\n\n\n\n\nFrom the plot, it is clear that the proportion of males among object search events is higher compared to females, suggesting that there is a relationship between the two variables.\nThis is confirmed thanks to the p-value displayed in the subtitle of the plot. As previously, we reject the null hypothesis and we conclude that the variables gender and stop and search events are not dependent (p-value = 0.038)."
  },
  {
    "objectID": "Fishersexactest.html#what-if-we-have-more-than-two-levels",
    "href": "Fishersexactest.html#what-if-we-have-more-than-two-levels",
    "title": "Fisher’s Exact Test",
    "section": "What if we have more than two levels?",
    "text": "What if we have more than two levels?\nUsing the drug example used previously lets say we have 3 drugs ‘Drug A, Drug B or Drug C’ and we want to see if there is any relationship with gender ‘Male/Female’.\n\n\nCode\n# Creating a data frame\n df = data.frame (\n   \"Drug\" = c(\"Drug A\", \"Drug B\", \"Drug A\", \"Drug C\", \"Drug C\"),\n   \"Gender\" = c(\"Male\", \"Male\", \"Female\", \"Female\", \"Female\")\n )\n \n# Creating contingency table using table()\n ctable = table(df)\n print(ctable)\n\n\n        Gender\nDrug     Female Male\n  Drug A      1    1\n  Drug B      0    1\n  Drug C      2    0\n\n\n\n\nCode\n# Running the Fisher's Exact test for the 3x2 table\nfisher.test(ctable)\n\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  ctable\np-value = 0.6\nalternative hypothesis: two.sided\n\n\nThe p-value is non-significant [p = 0.6], we fail to reject the null hypothesis (p &lt; 0.05) and conclude that there is no association between the drug treatments and gender. If the results had been significant we would have gone ahead and conducted a pair wise comparison."
  },
  {
    "objectID": "Fishersexactest.html#references",
    "href": "Fishersexactest.html#references",
    "title": "Fisher’s Exact Test",
    "section": "References",
    "text": "References\n\nBower, Keith M. 2003. “When to Use Fisher’s Exact Test.” In American Society for Quality, Six Sigma Forum Magazine, 2:35–37. 4.\nMcCrum-Gardner, Evie. 2008. “Which Is the Correct Statistical Test to Use?” British Journal of Oral and Maxillofacial Surgery 46 (1): 38–41.\nWong KC. Chi squared test versus Fisher’s exact test. Hong Kong Med J. 2011 Oct;17(5):427\nPatil, I. (2021). Visualizations with statistical details: The ‘ggstatsplot’ approach. Journal of Open Source Software, 6(61), 3167, doi:10.21105/joss.03167\nZach Bobbit. (2021). Fisher’s Exact Test: Definition, Formula, and Example\nBobbitt, Z. (2020). “Fisher’s Exact Test: Definition, Formula, and Example.” statology.org"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Generalized Linear Model: Using Count Data",
    "section": "",
    "text": "Code\nlibrary(katex)\nlibrary(gt)\nlibrary(gtsummary)\nlibrary(devtools)\nlibrary(pscl)\n#library(MEPS)\nlibrary(MASS)\nlibrary(tidyverse)"
  },
  {
    "objectID": "index.html#libraries-used",
    "href": "index.html#libraries-used",
    "title": "Generalized Linear Model: Using Count Data",
    "section": "",
    "text": "Code\nlibrary(katex)\nlibrary(gt)\nlibrary(gtsummary)\nlibrary(devtools)\nlibrary(pscl)\n#library(MEPS)\nlibrary(MASS)\nlibrary(tidyverse)"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Generalized Linear Model: Using Count Data",
    "section": "2 Introduction",
    "text": "2 Introduction\nIn this presentation, we will be discussing how to use the Generalized Linear Model (GLM) method using count data in the R programming language. We will show how to clean and wrangle the data, show necessary columns to execute our method, discuss assumptions held, showcase the code used to run this GLM, and the interpretation of our output results."
  },
  {
    "objectID": "index.html#what-is-a-glm",
    "href": "index.html#what-is-a-glm",
    "title": "Generalized Linear Model: Using Count Data",
    "section": "3 What is a GLM?",
    "text": "3 What is a GLM?\n\n3.1 lets start with a quick overview of the simple linear regression\nAs you know, a simple linear regression has two major components: a Y, dependent outcome and an X, which is your independent or your predictor variable. the model looks something like this:\nY_i = \\beta_0 + \\beta_1X_i + \\epsilon where \\beta_0 is your intercept and \\beta_1 is your slope. A linear model is a function, that us used to fit a data. We often use this method to see the association, or strength in association between two variables of interest:\nwhere:\n\nY_i is your dependent variable of interest\n\\beta_0 is your constant terms\n\\beta_1 is your slope coefficient\nand X_i is your independent variable\n\n\n\nCode\n# load data:\ndata(trees)\n\n#rename variables:\nnames(trees) &lt;- c(\"DBH_in\",\"height_ft\", \"volume_ft3\")\n\n# simple model:\nmodel &lt;- lm(DBH_in ~ height_ft, data = trees)\n\n# plot:\nsimple_trees &lt;-\n  #data\n  ggplot(data = trees) +\n  \n  # x and y\n  aes(x = height_ft, y = DBH_in) + \n  \n  #labels\n  labs(title = \"Example of Simple Association - Using Trees Data\",\n       x = \"Height in feet\",\n       y = \"Diameter in inches\") + \n  \n  # add points\n  geom_point() + \n  \n  # add the lm\n  geom_smooth(method = \"lm\", color = \"blue\", se = FALSE) +\n  \n  # add a simple theme\n  theme_bw()\n\n# actual plot: \nsimple_trees\n\n\n\n\n\nOften you will find this model writen in this form:\n E(Y|X) = \\beta_0 + \\beta_1 + \\varepsilon where \\beta_0 and \\beta_1 are our coefficients that need to be estimated and \\varepsilon, or the error term, is used for more complex lines. Our goal is to see the association between an outcome with an exposure.\n\n\n3.2 Assumptions of a linear equation\nBefore diving into the generalized models, lets quickly overview the assumptions of linear regression.\n\nAssumption 1: for each combination of independent variable x, y is a random variable with a certain probability distribution.\nAssumption 2: Y values are statistical dependent.\nAssumption 3: the mean value of Y, for each specific combination of values of X, is a linear function.\nAssumption 4: the variance of Y, is the same for any fixed value of X_n\nAssumption 5: for any fixed combination of X_n, Y is normally distributed.\n\nSomething to note, that in the simple explanation above we are assuming our Y variable (remember one of the points we talked above above? that y holds a specific distribution!) is continuous. So lets talk about our Y variable having a count distribution.\n\n\n3.3 lets talk about the generalized linear model\nthe term “generalized” is a big umbrella term used to describe a large class of models. Our response variable y_i is following an exponential family distribution with a mean of u_i which is sometimes non-linear! However McCallagh and Nelder considered them to be linear because our covariate affect the distribution of y_i only through linear combination.\nthere are three major components of a GLM:\n\nA Random component: which specifies the probability distribution of our response variable\nSystematic component: specifies the explanatory variable (x_1 .. x_n) in our model. Or their linear combination (\\beta_0 + \\beta_1) etc.\na link function: specifies the LINK between the random and systematic components. This helps us figure out our expected values of the response is related to the linear combination of our explanatory variables. for example the link function g(u) = u, which is called an identity function. this models the mean directly. Or, in the case we will talk about today the log of the mean:\n\n log(\\pmb{\\mu}) = \\alpha + \\beta_1x_1 + ... B_nx_n + \\epsilon Generalized linear function also follows certain assumptions:\n\nthe data points are Y_1, Y_2, ..., Y_n are independently distributed (cases are independent)\nGLM does not assume linear relationship between the response variable and explanatory variable but it does assume a linear relationship between the transformed expected response in terms of your link function\nExplanatory variables can be nonlinear transformation of some original variable\nParameter estimation uses maximum likelihood estimates (MLE rather than ordinary least squares)\nHomogeneity of variances (can check by calculating the variance at each level of your factor.)\nNormality of residuals\n\n\n\n3.4 What is a Poisson Regression?\na Poisson regression models how the mean of a discrete (or we can say count too!) response variable Y depends on our explanatory X variables. Here is a simple look at the Poisson regression:\n  log \\lambda_i = \\beta_0 + \\beta x_i  where the random component: the distribution of Y is the mean of \\lambda and the systematic component is the explanatory variable (or your X variables, which can be continuous or categorical) that is linearly associated. Or can be transformed if non-linear, and the link function is the log link stated in the section above.\nthis Poisson distribution follows a formula that looks as followed:\n P(X = x) {\\lambda^x e^{-\\lambda} \\over x!} where\n\nP(X = x) probability of (x) occurrences in a given interval\n\\lambda mean number of occurrences during interval\nx is the number of occurrences desired\ne is the base of natural logarithm\n\nAn advantage of using GLM over a normal line model is the link function gives us more flexibility in modeling and this model uses the Maximum likelihood estimate. Additionally we can use different inference tools like Wald’s test for logistic and Poisson models.\n\n\n3.5 important points of Poisson models\n\nwe can use this type of link function by using count data. count data can be describes at the number of devices that can access the internet, number of sex partners you have in your lifetime, and the number of individuals infected with a disease.\nPoisson is unimodel and skewed to the right, both the mean and the variance are the same. In other words, when the count is larger it tends to be more varied.\nif our mew increases the skew decreases and the distribution starts to become more bell shapped. our mean tends to look something like this:\n\n\\pmb{\\mu} = exp(\\alpha + \\beta x) = e^\\alpha (e^\\beta)^x  where one unit increase in X has a multiplicative impact on your e^\\beta power on the mean. (More on this a little later in the interpretation section!)\n\n\n3.6 A few last points before we move on\nLike with different models in statistics, one must follow the assumptions of a regression, and yes, Poisson has them as well. The assumptions for a Poisson regression are as follows:\n\nYour Y (dependent values) must be count values.\nCount values must have a positive integer (or who numbers) (0, 1, 2, 3, ... k). Negative values will not work here.\nexplanatory variables can be continuous, dichotomous or ordinal\nOur count variable must follow a Poisson distribution that is, the mean and variance should be the same. remember this one!\n\n \\pmb{\\mu} = E(X) = \\lambda    \\sigma^2 = \\lambda \nwhen your modeling count data, the link scale is linear. So the effects are additive on the link. While your response scale is nonlinear (this is on the exponent) and so the effects are multiplicative. makes sense? we will work out an example now!"
  },
  {
    "objectID": "index.html#example-1-using-glm-for-count-responses---using-the-meps-data-set-for-2020",
    "href": "index.html#example-1-using-glm-for-count-responses---using-the-meps-data-set-for-2020",
    "title": "Generalized Linear Model: Using Count Data",
    "section": "4 Example 1: using GLM for count responses - Using the MEPS data set for 2020",
    "text": "4 Example 1: using GLM for count responses - Using the MEPS data set for 2020\nLets import the built in National Health Care Survery data set from the Medical Expenditure Panel Survey website located here. the code book can also be located here for the 2020 Full year Consolidate data file. the MEPS is the medical expenditure panel survey data, which is a large scale survey about families and individual medical coverage across the United States.\n\n4.1 import MEPS data set\n\n\nCode\n# clean \nHC2020_clean &lt;- read_csv(\"/Users/anabravo/Documents/GitHub/R-health-blog/HC2020_clean.csv\")\n\n# Load data from AHRQ MEPS website\nhc2020 = read_MEPS(file = \"h224\")\n\n# lets name a copy of this data set so i don't ruin it\nhc2020_2 &lt;- hc2020\n\n# these variables are upper case, lets turn them lower case\nnames(hc2020_2) &lt;- tolower(names(hc2020_2))\n\n# theres about 14,000 variables. lets keep the ones we want to look at\nhc2020_subset &lt;- hc2020_2 |&gt; \n  select(dupersid,obdrv20, ertot20, rthlth31, adpain42, region31, age20x, racev1x, sex, marry31x, educyr, faminc20, empst31, mcare20)\n\n\n\n# cleaning names:--------------------------------------------------------------\n# dupersid = Person ID\n# obdrv20 = number of office based physician visits in 2020 \n# ertot20 = emergency rooms visits in 2020\n# rthlth31 = perceived health status \n# adpain41 = pain level \n# region31 = Census region: Northest, midwest, south and west \n# age20x = Age as of Dec 2020\n# racev1x = race: white, black, American indiain, Asian, multiple races \n# sex = sex at birth \n# marry31x = marital status\n# educyr = year of ed when entered in MEPS\n# faminc20 = family total income \n# empst31 = employment status \n# mcare20 = Covered by medicare? Yes/no \n\n# removing old names because that sub header in columns messes me up sometimes ---\n\nnames(hc2020_subset) &lt;- NULL\n\n\nnew_names &lt;- c(\"Person_ID\",\n               \"Nubr_office_visits\",\n               \"Nubr_emergency_visits\",\n               \"Health_status\",\n               \"Pain_Level\",\n               \"Region\",\n               \"Age_as_Dec2020\",\n               \"Race\",\n               \"Is_male\",\n               \"Is_married\",\n               \"Education_lvl\",\n               \"Total_fam_incom\",\n               \"Is_employed\",\n               \"Covered_by_MediCar\")\n\n\nnames(hc2020_subset) &lt;- new_names\n\n\n# this data set has codes for NA, will switch to NA in R ------------------------\n\nHC2020_clean &lt;- HC2020_clean |&gt; \n  mutate(Pain_Level = na_if(Pain_Level, -15)) |&gt;\n  mutate(Health_status = na_if(Health_status, -8)) |&gt;\n  mutate(Region = na_if(Region, -1)) |&gt; \n  mutate(Age_as_Dec2020 = na_if(Age_as_Dec2020, -1)) |&gt;\n  mutate(Is_married = na_if(Is_married,-8 )) |&gt; \n  mutate(Is_employed = na_if(Is_employed, -15)) |&gt; \n  mutate(Education_lvl = na_if(Education_lvl, -15)) |&gt; \n  mutate(Covered_by_MediCar = na_if(Covered_by_MediCar, -1))\n  \n  \n# also, for ease of interpretation i will dichotomize some variables ------------\n\n\n HC2020_clean &lt;- HC2020_clean |&gt; \n  mutate(Is_married = ifelse(Is_married == 1, 1, 0)) |&gt; \n  mutate(Is_employed = ifelse(Is_employed == 1, 1, 0)) |&gt; \n  mutate(Is_male = ifelse(Is_male == 1 , 1, 0 )) |&gt; \n  mutate(Covered_by_MediCar = ifelse(Covered_by_MediCar == 1, 1, 0))\n\n\n\n# i also want to transform some variable using case when -----------------------\n\nHC2020_clean &lt;- HC2020_clean |&gt; \n  mutate(Health_status = case_when(\n    Health_status == 1 ~ \"Good\",\n    Health_status == 2 ~ \"Good\",\n    Health_status == 3 ~ \"Fair\",\n    Health_status == 4 ~ \"Fair\",\n    Health_status == 5 ~ \"Poor\"\n    )) \n\n# i want to write CSV this data set for later use --------------------------------\n\nwrite_csv(hc2020_subset,\n          file = \"/Users/anbravo/GitHub/R-health-blog/HC2020_clean.csv\")\n\n\nlet’s say i want to take a sneak peak at this data set. I will use the gtsummary package to look at the first levels of the data set, and I just want to very simply make this data set interactive by using the opt_interactive() function in the gt package.\n\n\nCode\nHC2020_clean &lt;- read_csv(\"/Users/anabravo/Documents/GitHub/R-health-blog/HC2020_clean.csv\")\n\nHC2020_clean |&gt; \n  select(Person_ID, Nubr_office_visits, Health_status, Pain_Level, Region, Is_employed, Is_married, Is_employed, Is_male) |&gt; \n  #head(10) |&gt; \n  gt() |&gt; \n  \n  tab_header(\n    title = \"Brief view of HC 2020 data set\",\n    subtitle = \"2020 Data from MEPS website\"\n  ) |&gt; \n  \n opt_interactive(\n   use_search = TRUE,\n   use_highlight = TRUE, \n   use_compact_mode = TRUE,\n   use_resizers = TRUE,\n   use_text_wrapping = FALSE,\n   pagination_type = \"jump\"\n   )\n\n\n\n\n\n\nBrief view of HC 2020 data set\n2020 Data from MEPS website"
  },
  {
    "objectID": "index.html#some-quick-data-exploration",
    "href": "index.html#some-quick-data-exploration",
    "title": "Generalized Linear Model: Using Count Data",
    "section": "5 Some quick data exploration",
    "text": "5 Some quick data exploration\nRemember what I said earlier about the mean and variance being the same? EDA is important because it might tell us more details about our variable of interest. For this particular question, we might be interested in the number of times an event has occurred. In this case, the number of times someone pay a visit to an office doctor. This data follows everyone for exactly one year.\n\nI am interested in seeing the number of physician office visits in the last year (we can also call this our Y variable)\nWe are interested in seeing how self-perceived health status (Good, Fair, or Poor), gender (is male, 1 = male, 0 = female), and marital status (married = 0, not married/other categories = 0) is associated with number of doctor visits (our X variables).\n\nBecause we are looking at count data, we might find it important to check out the mean and variance of our Y variables. In the case that our Y variable mean and variance are not the same, we might consider some sort of transformation. Transforming a data set can help with the normality of it.\n\n5.1 lets look at the outcome variable of interest\n\n\nCode\nmean(HC2020_clean$Nubr_office_visits, na.rm = TRUE)\n\n\n[1] 2.792591\n\n\nthe mean of this data set is about\n\n\nCode\nvar(HC2020_clean$Nubr_office_visits, na.rm = TRUE)\n\n\n[1] 34.58851\n\n\nand the variance is about 34.59. We might need to do something with this particular data set but as an example, lets first run a Generalized Linear Model without doing any kind of transformation to the data.\n\n\n5.2 Checking the distribution of the data\n\n\nCode\nhist(\n  x = HC2020_clean$Nubr_office_visits,\n  main = \"Distrubtion of Number of Office visits\",\n  xlab = \"Office visits within a year\"\n)\n\n\n\n\n\nI might also just be interested in the general responses of our y variable of interest:\n\n\nCode\n# I'm just interested in the frequency \n\nHC2020_clean |&gt; \n  group_by(Nubr_office_visits) |&gt; \n  summarise(n = n()) |&gt; \n  mutate(Freq = n / sum(n), Freq = scales::percent(Freq)) |&gt; \n  gt() |&gt; \n  opt_interactive(\n   use_search = TRUE,\n   use_highlight = TRUE, \n   use_compact_mode = TRUE,\n   use_resizers = TRUE,\n   use_text_wrapping = FALSE,\n   pagination_type = \"jump\"\n   )\n\n\n\n\n\n\n\n\n\n\nWe might need to check if we have over dispersion in this model. A quick way to check is by diving the residual deviance by our degrees of freedom. IF this value is greater than one, then we can use probably use a negative binomial distribution."
  },
  {
    "objectID": "index.html#how-to-build-a-glm-with-a-poisson-distribution",
    "href": "index.html#how-to-build-a-glm-with-a-poisson-distribution",
    "title": "Generalized Linear Model: Using Count Data",
    "section": "6 How to build a GLM with a Poisson Distribution",
    "text": "6 How to build a GLM with a Poisson Distribution\nThe basic core functions of a GLM look something like this: glm(counts ~ outcome + treatment, data = data_set, family = \"poisson\") which includes:\n\ncounts: would be your Y variable\ndata: where your pulling the data from\n~: or an equal sign\noutcome + treatment: are your X variables\nfamily: the link or distribution you are following\n\nWe are interested in measuring number of visits to a doctors office as a function of perceived health, sex assigned at birth, and marital status.\n\n# build first model \nPossionModel1 &lt;- glm(Nubr_office_visits ~ Health_status + Is_male + Is_married, \n                     data = HC2020_clean, family = \"poisson\")\n\n# check summary \nsummary(PossionModel1)\n\n\nCall:\nglm(formula = Nubr_office_visits ~ Health_status + Is_male + \n    Is_married, family = \"poisson\", data = HC2020_clean)\n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    0.082451   0.015568   5.296 1.18e-07 ***\nHealth_status  0.354521   0.003092 114.674  &lt; 2e-16 ***\nIs_male        0.268414   0.007327  36.633  &lt; 2e-16 ***\nIs_married    -0.111176   0.001788 -62.191  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 167475  on 27804  degrees of freedom\nResidual deviance: 147000  on 27801  degrees of freedom\nAIC: 195696\n\nNumber of Fisher Scoring iterations: 6\n\n\nthis summary gives us the null deviance, which is the total sum of squares, and the residual deviance which is the sum of square errors or the unexplained deviance. We also get the AIC for this model (although AIC is more useful when comparing to other model fits) and we also get the model coefficients for Health status, sex assigned at birth, and marital status. We also get an AIC score of 199315.\nlets divide the residual deviance by the degrees of freedom.\n\n\nCode\nPossionModel1$deviance / PossionModel1$df.residual\n\n\n[1] 5.287586\n\n\nthe value we got from this was 5.56. this value is very big and so we should consider a negative binomial regression.\nAdditionally, this output is extremely ugly so we will feed this model into gtsummary in a little bit so looks a little better.\n\n6.1 Modeling Possion Model 1 with GT summary\n\n\nCode\nPossionModel1 |&gt; \n  tbl_regression(exponentiate = TRUE) |&gt; \n  bold_levels() |&gt;\n  bold_p(t = .1)\n\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      IRR1\n      95% CI1\n      p-value\n    \n  \n  \n    Health_status\n1.43\n1.42, 1.43\n&lt;0.001\n    Is_male\n1.31\n1.29, 1.33\n&lt;0.001\n    Is_married\n0.89\n0.89, 0.90\n&lt;0.001\n  \n  \n  \n    \n      1 IRR = Incidence Rate Ratio, CI = Confidence Interval"
  },
  {
    "objectID": "index.html#interpreting-rate-ratio",
    "href": "index.html#interpreting-rate-ratio",
    "title": "Generalized Linear Model: Using Count Data",
    "section": "7 Interpreting Rate Ratio",
    "text": "7 Interpreting Rate Ratio\nFor this particular data set, we are interested in looking at the number of doctor visits over a year. the coefficient for \\beta_1 = -0.579 and is statistically significant. Meaning that perceived health influence the rate of doctors visits. And because it is negative we can figure out by how much. So in other words e^{-.579} = 0.56. This is our rate ratio, the multiplicative increase on doctors visits for those perceived as good health when compared to fair. Because out \\beta is &lt; 1, then perceived good health has a protective factor and we can interpret this as:\nPerceived good health is associated with a 44% reduction (0.56 - 1 = -0.44) in paid doctors visits in a year.\nLets interpret perceived poor health status when compared to fair. the coefficient for this \\beta = 0.74. and is statistically significant. And because it is positive we can figure out by how much and the direction. So in other words e^{.74}) = 2.12.\nIn other words, those who perceived themselves as in poor health when compared to those perceived in fair health have a 2.12 times more doctors visit. Or we can also say, that perceived poor health is associated with an increase of 112% (2.12 - 1 = 112) increase in doctors visits."
  },
  {
    "objectID": "index.html#how-to-acount-for-overdispersion",
    "href": "index.html#how-to-acount-for-overdispersion",
    "title": "Generalized Linear Model: Using Count Data",
    "section": "8 How to acount for overdispersion",
    "text": "8 How to acount for overdispersion\nRemember how we spoke about earlier that this particular model we are fitting is most likely over dispersed? Well, we should try to account for that. Over dispersion means that we may have to much variation in an Poisson regression model. some ways to mediate this is by:\n\nselecting a different random component distribution that can account for this (like negative binomial)\nor use a nonlinear and generalize linear mixed model to handle the random effects.\n\nin this case we will go for negative binomial distribution."
  },
  {
    "objectID": "index.html#negative-binomial-distribution",
    "href": "index.html#negative-binomial-distribution",
    "title": "Generalized Linear Model: Using Count Data",
    "section": "9 Negative Binomial Distribution",
    "text": "9 Negative Binomial Distribution\nA negative binomial regression can be used for over-dispersed count data, like mentioned, when the variance exceed the mean. This can be considered like a generalization of the Poisson regression, because it has the same mean structure as Poisson regression but it has an extra parameter to model over-dispersion! Also because of the, the confidence intervals for a negative binomial may be a bit wider when compared to a normal Poisson regression model.\n\na note about Zero-inflated regressio models: this type of model attempt to account for to many zeros. Zero-inflated models estimate two models at the same time, one for count model and one for excess zeros.\n\n\nwe can also compare our model to a zero inflated model to check for fit:\n\n\n\nCode\n#this is prob a better fit the negative B   \nPoissonModel2_NB &lt;- glm.nb(Nubr_office_visits ~ Health_status + Is_male + Is_married, data = HC2020_clean)\n\n# fit model with log link\nPoissonModel3_ZI &lt;- zeroinfl(Nubr_office_visits ~ Health_status + Is_male + Is_married, data = HC2020_clean, dist = \"poisson\")\n\n#summary analysis\nsummary(PoissonModel3_ZI)\n\n\n\nCall:\nzeroinfl(formula = Nubr_office_visits ~ Health_status + Is_male + Is_married, \n    data = HC2020_clean, dist = \"poisson\")\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-1.9665 -0.9094 -0.6168  0.2381 65.8744 \n\nCount model coefficients (poisson with log link):\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    0.915156   0.016305   56.13   &lt;2e-16 ***\nHealth_status  0.258626   0.003243   79.74   &lt;2e-16 ***\nIs_male        0.119888   0.007561   15.86   &lt;2e-16 ***\nIs_married    -0.074818   0.001884  -39.71   &lt;2e-16 ***\n\nZero-inflation model coefficients (binomial with logit link):\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    0.447637   0.052250   8.567   &lt;2e-16 ***\nHealth_status -0.250294   0.011599 -21.578   &lt;2e-16 ***\nIs_male       -0.408658   0.025879 -15.791   &lt;2e-16 ***\nIs_married     0.084639   0.005891  14.367   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nNumber of iterations in BFGS optimization: 11 \nLog-likelihood: -8.033e+04 on 8 Df\n\n\nCode\n# comparing AIC score of ZB and NB\nAIC(PoissonModel2_NB, PoissonModel3_ZI)\n\n\n                 df      AIC\nPoissonModel2_NB  5 112878.1\nPoissonModel3_ZI  8 160681.8\n\n\nas suspected, the model PoissonModel2_NB negative binomial, seems to be a better fit.\nLet’s go with a negative binomial regression analysis. In this case we will need to utilize the MASS package to estimate negative binomial regression parameters and account for the dispersion in this data set.\n\n\nCode\nPoissonModel2_NB &lt;- glm.nb(Nubr_office_visits ~ Health_status + Is_male + Is_married, data = HC2020_clean)\n\nsummary(PoissonModel2_NB)\n\n\n\nCall:\nglm.nb(formula = Nubr_office_visits ~ Health_status + Is_male + \n    Is_married, data = HC2020_clean, init.theta = 0.4951876714, \n    link = log)\n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    0.108756   0.038393   2.833  0.00462 ** \nHealth_status  0.338842   0.008189  41.379  &lt; 2e-16 ***\nIs_male        0.292978   0.018897  15.504  &lt; 2e-16 ***\nIs_married    -0.120788   0.004340 -27.831  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(0.4952) family taken to be 1)\n\n    Null deviance: 30980  on 27804  degrees of freedom\nResidual deviance: 27891  on 27801  degrees of freedom\nAIC: 112878\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  0.49519 \n          Std. Err.:  0.00594 \n\n 2 x log-likelihood:  -112868.09800 \n\n\nlets check the dispersion of this negative binomial model\n\n\nCode\nPoissonModel2_NB$deviance / PoissonModel2_NB$df.residual\n\n\n[1] 1.003251\n\n\n1.00 is much better than 5. Also the AIC value for this model is 112172 which is much better than the original model fit we used on model 1. lets fit this model into GT summary:\n\n\nCode\nPoissonModel2_NB |&gt; \n  tbl_regression(exponentiate = TRUE) |&gt; \n  bold_levels() |&gt;\n  bold_p(t = .1)\n\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      IRR1\n      95% CI1\n      p-value\n    \n  \n  \n    Health_status\n1.40\n1.38, 1.43\n&lt;0.001\n    Is_male\n1.34\n1.29, 1.39\n&lt;0.001\n    Is_married\n0.89\n0.88, 0.89\n&lt;0.001\n  \n  \n  \n    \n      1 IRR = Incidence Rate Ratio, CI = Confidence Interval"
  },
  {
    "objectID": "index.html#interpretting-males-and-those-who-are-married-in-this-data-set",
    "href": "index.html#interpretting-males-and-those-who-are-married-in-this-data-set",
    "title": "Generalized Linear Model: Using Count Data",
    "section": "10 Interpretting Males and those who are married in this data set",
    "text": "10 Interpretting Males and those who are married in this data set\nOur reference group is 0, so for variables Is_male and Is_married the reference group in this case would be females and not married. When your fitting a model, it is very important to double check what is your reference group because different statistical software may default to different reference groups.\n\nIs_male: e^{-0.32} = 0.72, when comparing males to females, identifying as male is associated with a reduction of 28% (0.72 - 1 = -0.28) in doctors visits.\nIs_married: e^{.32} = 1.38, when compared married to non-married individuals, being married is associated with a 38% increase (1.38 - 1 = .38) in doctors visit. Or, the married group has a 1.38 times more doctors visit when compared to non-married."
  },
  {
    "objectID": "index.html#conclusion",
    "href": "index.html#conclusion",
    "title": "Generalized Linear Model: Using Count Data",
    "section": "11 Conclusion",
    "text": "11 Conclusion\nToday you learned of the important of linear regression, the benefits of a generalized linear model, and its assumptions what is a Poisson regression and the assumption that need to be satisfied to run this model, the importance of a Poisson regression, how to build a GLM with a Poisson link, how the clean data from MEPS, how to visualize some counts for this data, how to account for over dispersion and how to interpret rate ratio."
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Generalized Linear Model: Using Count Data",
    "section": "12 References",
    "text": "12 References\n\nMcCullagh, P., and J. A. Nelder. 2019. “An Outline of Generalized Linear Models,” January, 21–47. https://doi.org/10.1201/9780203753736-2\nNelder, J. A., and R. W. M. Wedderburn. 1972. “Generalized Linear Models.” Journal of the Royal Statistical Society. Series A (General) 135 (3): 370. https://doi.org/10.2307/2344614.\nGuyen, M. (2023). “A Guide on Data Analysis”. Made on Bookdown. https://bookdown.org/mike/data_analysis/poisson-regression.html\nBruin, J. (2011). Negative Bonomial Regression, R Dad Analysis Examples. https://stats.oarc.ucla.edu/stata/ado/analysis/"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Ana Bravo",
    "section": "",
    "text": "My name is Ana Bravo. I am Biostatistician by nature and a data analyst by heart who is interested in applying my vast knowledge in academic and community research into the health care field. My unique, strong, and well-rounded experience in statistical analysis, research, and community outreach not only makes me an ideal candidate, but my background allows me to make sound and quick decisions into a collaborative workforce and provide key insight in decision making. My hope for the future is to bridge gaps between scientific investigators and the community they research in the hopes to create a more accessible field for the betterment and advanced in open source science."
  },
  {
    "objectID": "RandomInterceptModel.html",
    "href": "RandomInterceptModel.html",
    "title": "Random Intercept Model",
    "section": "",
    "text": "Code\nlibrary(gt)        # to make table outputs presentation/publication ready. \nlibrary(broom)     # clean output.\nlibrary(knitr)     # to make tables. \nlibrary(gtsummary) # extension to gt\nlibrary(lattice)   # It is a powerful data visualization for multivariate data\nlibrary(lme4)      # Fit linear and generalized linear mixed-effects models\nlibrary(arm)       # Data Analysis Using Regression and Multilevel models\nlibrary(tidyverse) # for cleaning wrangling data"
  },
  {
    "objectID": "RandomInterceptModel.html#libraries-used",
    "href": "RandomInterceptModel.html#libraries-used",
    "title": "Random Intercept Model",
    "section": "",
    "text": "Code\nlibrary(gt)        # to make table outputs presentation/publication ready. \nlibrary(broom)     # clean output.\nlibrary(knitr)     # to make tables. \nlibrary(gtsummary) # extension to gt\nlibrary(lattice)   # It is a powerful data visualization for multivariate data\nlibrary(lme4)      # Fit linear and generalized linear mixed-effects models\nlibrary(arm)       # Data Analysis Using Regression and Multilevel models\nlibrary(tidyverse) # for cleaning wrangling data"
  },
  {
    "objectID": "RandomInterceptModel.html#what-is-a-random-intercept-model",
    "href": "RandomInterceptModel.html#what-is-a-random-intercept-model",
    "title": "Random Intercept Model",
    "section": "2 What is a Random Intercept model",
    "text": "2 What is a Random Intercept model\nBefore talking about a random intercept model, let’s understand why they are necessary and important in the real world by discussing a variance component model first. This will make sense as we go along in this lecture.\n\n2.1 Variance component model\nWe are familiar with a fixed level of a factor or variable. Which means that the factor level in an experiment is the only thing we are interested. For example, let’s say we are interested in measuring the difference in resistance resulting from putting identical resistors to three different temperatures for a period of 24 hours. Let’s say we have three different groups, and each of these three different groups have a sample size of 5. So each of the three treatment groups was replicated 5 times.\n\n\n\nLevel 1\nLevel 2\nLevel 3\n\n\n\n\n6.9\n8.3\n8.0\n\n\n5.4\n6.8\n10.5\n\n\n5.8\n7.8\n8.1\n\n\n4.6\n9.2\n6.9\n\n\n4.0\n6.5\n9.3\n\n\nmean\nmean\nmean\n\n\n5.34\n7.72\n8.56\n\n\n\nIn this example, the level of the temperature is considered fixed meaning, the three temperatures were the only ones that we are interested in. This is called a fixed effects model.\na fixed effect model is a statistical model in which the parameters are fixed or non-random. This can also be referred to a regression model, in which group mean are “fixed” (non-random) or in simpler, terms something that is “fixed” in analysis is constant like sex assigned at birth or ethnicity.\n y_i = \\beta_0 + X_i\\beta_i + \\alpha_u + \\epsilon_i \nNow, let’s say we want to look at different levels of factors that were chosen because of random sampling, like number of operators working that day, lot batches, days etc. So in this case we are now regarding factors not related to themselves (variables) but we are now trying to represent all possible levels that these factors may take, the appropriate model is now a random effects model.\nfitting these random effects models are important because we want to obtain estimates of different contributions that experimental factors make to the variability of our data! (we can represent this as the variance) this is what is called variance component\n\n\n2.2 Why this is relevant\nWell, a variance component model helps us see how much variance in our response at the different levels. But what if you are interested in seeing the effects of the explanatory variables? Or, what if your observations are NOT randomly sampled from simple random sample but instead from a cluster or a multi-level sampling design? Random intercept models or random effects models are important.\n\n\n2.3 Example 1: School level data\nLet’s say we have some data on exam results of students within a school and we use a variance component model and see that 15% of the variance is at the school level. Like for example, differing school districts, differing school policies etc. However, is it fair to really say that 15% of the variance in example scores is caused by schools? you could also say that maybe that part of the variance could be cause by the students being different themselves as well before taking the exam.\nIn this case, it might be important to control for the previous exams the students took, so you can look at the variance that is due to the things that happened when the students were at that school."
  },
  {
    "objectID": "RandomInterceptModel.html#fitting-a-single-level-regression-model",
    "href": "RandomInterceptModel.html#fitting-a-single-level-regression-model",
    "title": "Random Intercept Model",
    "section": "3 Fitting a single-level regression model",
    "text": "3 Fitting a single-level regression model\nwhen we want to control for something (like previous exams students took) we can fit a single-level regression model that looks something like this:\ny_1 = \\beta_0 + \\beta_1x_i + e_i where\n\ny_1 is your dependent variable\n\\beta_0 is your intercept and\n\\beta_1 is your slope parameter (which is also your slope treatment effect).\n\nWhen you have clustered data fitting this model causes problems. Clustered data is data where you observation or participants are related. Like exam results for students within a school, height of children within a family etc.\nif we try to fit this clustered data:\n\nour standard errors will be wrong.\nthis single level data model doesn’t show up how much variation is at the school level and how much much of the variation is at the student level.\n\nSo fitting this type of data in this regression we wont know how much of an effect the school level has on the exam score, after controlling for the previous score."
  },
  {
    "objectID": "RandomInterceptModel.html#solution-fitting-a-random-intercept-model",
    "href": "RandomInterceptModel.html#solution-fitting-a-random-intercept-model",
    "title": "Random Intercept Model",
    "section": "4 Solution: Fitting a Random Intercept model",
    "text": "4 Solution: Fitting a Random Intercept model\nSo what we can do is combine the variance component and single-level regression model to build a random intercept model. So this random intercept model has 2 random terms. the level one random term: e_{ij} \\sim N(0, \\sigma_e^2) and the N(0, \\sigma_u^2) and has two parts:\n\na fixed part\na random part\n\n y_{ij} = \\overbrace{\\beta_0 + \\beta_1X_{ij}}^{\\text{fixed part}} + \\underbrace{u_j + e_{ij}}_{\\text{random part}} where the fixed parts includes our parameters that we estimate as our coefficients, and the random part is the parameter we estimate as the variance e_{ij} \\sim N(0, \\sigma_e^2) and the N(0, \\sigma_u^2) and these are allowed to vary and u_j and e_{ij} are normally distributed.\nwhere:\n\ny_{ij} is your dependent variable at i individual and j level\nN(0, \\sigma_u^2) is the measurement at the school level\nand e_{ij} \\sim N(0, \\sigma_e^2) is the measurement at the student level\nand i subscript is for the students\nand j is the school subscript\n\nwe can also write this equation like so:\n Y_{ij} = \\mu + b_i + \\varepsilon_{ij} \nwhere\n\nY_{ij} is your dependent outcome of intested for a subject i at school j\n\\mu is the population average mean\nb_i is the random students effects (you have a random effect for every student)\n\\varepsilon_{ij} is your random error."
  },
  {
    "objectID": "RandomInterceptModel.html#final-key-points",
    "href": "RandomInterceptModel.html#final-key-points",
    "title": "Random Intercept Model",
    "section": "5 Final key points",
    "text": "5 Final key points\n\nrandom intercept models are used for answering questions about clustered data, and at different levels. For example, what is the relationship between exam scores at 11 and at age 16? how much variation is there between students progress from 11 to 16 at the school level?\nb_i is the error associated with the students.\n\\varepsilon_{ij} is the random error.\nfor a random intercept model, each individual will have a random intercept, but the sample slope."
  },
  {
    "objectID": "RandomInterceptModel.html#assumptions-of-a-random-effects-model",
    "href": "RandomInterceptModel.html#assumptions-of-a-random-effects-model",
    "title": "Random Intercept Model",
    "section": "6 Assumptions of a random effects model:",
    "text": "6 Assumptions of a random effects model:\n\nunobserved cluster effects is not correlated with observed variables (all u_{ij} terms are not correlated with the your predictors.)\nthe within and between effects are the same.\nyour error term is independent with your constant term.\nyou have homoscedasticity\nb_i and \\varepsilon are independent of each other"
  },
  {
    "objectID": "RandomInterceptModel.html#hypothesis-of-a-random-effects-model",
    "href": "RandomInterceptModel.html#hypothesis-of-a-random-effects-model",
    "title": "Random Intercept Model",
    "section": "7 hypothesis of a random effects model:",
    "text": "7 hypothesis of a random effects model:\nhypothesis testing for a random effects model runs as follows:\n H_0: \\sigma^2_u = 0 H_1: \\sigma^2_u \\not = 0 the null hypothesis states that if \\sigma^2_u is true, then the random component is not needed in this model. so you can fit a single level regression model. to do this, you would can do a likelihood ratio test comparing the two model to see if sigma is significant. In other words, seeing if there is no difference in intercepts. If there is NO difference in intercepts (or the slopes are similar), then a random intercept model or random component is not needed."
  },
  {
    "objectID": "RandomInterceptModel.html#example-2-planktonic-larval-duration-pld",
    "href": "RandomInterceptModel.html#example-2-planktonic-larval-duration-pld",
    "title": "Random Intercept Model",
    "section": "8 Example 2: Planktonic larval duration (PLD)",
    "text": "8 Example 2: Planktonic larval duration (PLD)\nthis is example is from O’Connor et al (2007). A brief intro on this study, temperature is important for the development of organisms. In marine species, temperature is very important and linked to growth. So the time spent as a planktonic larvae can have associations on mortality and regulation on the species. Previous research has looked at the association between species comparison but not within species comparisons. What if we are interest in within and between species variation?\n\n8.1 load PLD data\n\n\nCode\nPLD &lt;- read_table(\"/Users/anabravo/Documents/GitHub/R-health-blog/PLD.txt\")\n\n\nI am curious about the structure of this data and how it briefly looks.\n\n\nCode\n#strcuture \nstr(PLD)\n\n\nspc_tbl_ [214 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ phylum : chr [1:214] \"Annelida\" \"Annelida\" \"Annelida\" \"Annelida\" ...\n $ species: chr [1:214] \"Circeus.spirillum\" \"Circeus.spirillum\" \"Circeus.spirillum\" \"Hydroides.elegans\" ...\n $ D.mode : chr [1:214] \"L\" \"L\" \"L\" \"P\" ...\n $ temp   : num [1:214] 5 10 15 15 20 25 30 5 10 17 ...\n $ pld    : num [1:214] 16 16 4 8 6.5 5 3.2 15 9.5 4.5 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   phylum = col_character(),\n  ..   species = col_character(),\n  ..   D.mode = col_character(),\n  ..   temp = col_double(),\n  ..   pld = col_double()\n  .. )\n\n\nCode\n# just the top - seeing how it looks\nhead(PLD)\n\n\n# A tibble: 6 × 5\n  phylum   species           D.mode  temp   pld\n  &lt;chr&gt;    &lt;chr&gt;             &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 Annelida Circeus.spirillum L          5  16  \n2 Annelida Circeus.spirillum L         10  16  \n3 Annelida Circeus.spirillum L         15   4  \n4 Annelida Hydroides.elegans P         15   8  \n5 Annelida Hydroides.elegans P         20   6.5\n6 Annelida Hydroides.elegans P         25   5  \n\n\nCode\n# brief summary\nsummary(PLD)\n\n\n    phylum            species             D.mode               temp      \n Length:214         Length:214         Length:214         Min.   : 2.50  \n Class :character   Class :character   Class :character   1st Qu.:12.28  \n Mode  :character   Mode  :character   Mode  :character   Median :20.00  \n                                                          Mean   :18.59  \n                                                          3rd Qu.:25.00  \n                                                          Max.   :32.00  \n      pld        \n Min.   :  1.00  \n 1st Qu.: 11.00  \n Median : 19.30  \n Mean   : 26.28  \n 3rd Qu.: 33.45  \n Max.   :129.00  \n\n\nI am curious about how this would look just plotting the variable pld or planktonic larvae duration and the temperature. So i am interested in seeing how the temperature is associated with their their survival duration.\n\n\nCode\n# how to plot in base R \nplot(pld ~ temp, data = PLD,\n     xlab = \"temperature (C)\",\n     ylab = \"PLD survival (Days)\",\n     pch = 16)\n# add lm line in base R\nabline(lm(pld ~ temp, data = PLD))\n\n\n\n\n\nyou can also do this in ggplot plot like so:\n\n\nCode\nggplot(data = PLD) +\n  aes(y = pld, x = temp) +\n  stat_smooth(method = \"lm\") +\n  geom_point() +\n  theme_bw()\n\n\n\n\n\n\n\n8.2 Fit first model: Linear model\nLet’s first fit a linear model and check any assumptions. Why are we fitting a linear model first? It might be important to check the standard error of this model and compare to the next model, that might be a better fit later on.\n\n\nCode\n# fitting linear model first \n\nLinearModel_1 &lt;- lm(pld ~ temp, data = PLD)\n\nsummary(LinearModel_1)\n\n\n\nCall:\nlm(formula = pld ~ temp, data = PLD)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-44.158 -11.351  -0.430   7.684  93.884 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  54.8390     3.7954  14.449  &lt; 2e-16 ***\ntemp         -1.5361     0.1899  -8.087 4.65e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.36 on 212 degrees of freedom\nMultiple R-squared:  0.2358,    Adjusted R-squared:  0.2322 \nF-statistic:  65.4 on 1 and 212 DF,  p-value: 4.652e-14\n\n\nWe can fit this output in a gtsummary to make it nicer looking:\n\n\nCode\nLinearModel_1 |&gt; \n  tbl_regression()\n\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    temp\n-1.5\n-1.9, -1.2\n&lt;0.001\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nI am interested in checking out visually, the equal variance (homoscedasticity) and so i will plot a a base residual graph:\n\n\nCode\n# better to do a scatter plot \nLinearModel_res &lt;- resid(LinearModel_1)\n\n# plot residual \nplot(PLD$temp, LinearModel_res,\n     ylab = \"residuals\",\n     xlab = \"temp\",\n     main = \"Residual graph\"\n     )\nabline(0,0)\n\n\n\n\n\nJust upon visual observation, it seems like this assumption may be violated so i think it might be important to do some transformations.\n\n\n8.3 Log transformation\n\n\nCode\nLinearMode_2Log &lt;- lm(log(pld) ~ log(temp), data= PLD)\n\nsummary(LinearMode_2Log)\n\n\n\nCall:\nlm(formula = log(pld) ~ log(temp), data = PLD)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0768 -0.3956  0.1802  0.5461  1.9656 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   4.6946     0.3128  15.011  &lt; 2e-16 ***\nlog(temp)    -0.6308     0.1093  -5.771 2.77e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8236 on 212 degrees of freedom\nMultiple R-squared:  0.1358,    Adjusted R-squared:  0.1317 \nF-statistic: 33.31 on 1 and 212 DF,  p-value: 2.767e-08\n\n\n\n\n8.4 residual of new log transformed graph\n\n\nCode\n# better to do a scatter plot \nLinearModel2_res &lt;- resid(LinearMode_2Log)\n\n# plot residual \nplot(PLD$temp, LinearModel2_res,\n     ylab = \"residuals(log)\",\n     xlab = \"temp(log)\",\n     main = \"Residual graph (log transformation)\"\n     )\nabline(0,0)\n\n\n\n\n\nA bit better! Now i kinda want to see the original plot i plotted with PLD and temperature:\n\n\nCode\nplot(log(pld) ~ log(temp), data = PLD,\n     xlab = \"Temperature in C\",\n     ylab = \"PLD in days\")\nabline(LinearMode_2Log)\n\n\n\n\n\nin ggplot you can use the facet_wrap() function to separate by phylum:\n\n\nCode\nggplot(data = PLD) +\n  aes(x = log(temp), y = log(pld)) +\n  geom_point() +\n  stat_smooth(method = \"lm\") +\n  facet_wrap(~phylum) +\n  theme_classic()"
  },
  {
    "objectID": "RandomInterceptModel.html#fitting-a-random-intercept-model-random-intercept-same-slope",
    "href": "RandomInterceptModel.html#fitting-a-random-intercept-model-random-intercept-same-slope",
    "title": "Random Intercept Model",
    "section": "9 Fitting a random intercept model (random intercept, same slope)",
    "text": "9 Fitting a random intercept model (random intercept, same slope)\nI am interested in seeing if the overall temperature and the PLD relationship is similar among species, but not the same. We are interested in plotting a mixed effects model with a random intercept but fixed/same slope. I am only interested in the species-specific plot for now with the phylum Mollusca.\n\n\nCode\n# filter to only mollusca\n\nMollusca_subset &lt;- \n  PLD |&gt; \n  filter(phylum == \"Mollusca\")\n\nggplot(data = Mollusca_subset) +\n  aes(x = log(pld), y = log(temp)) +\n  geom_point() +\n  labs(x = \"Log(temperature)\",\n       y = \"Log(PLD)\") + \n  stat_smooth(method = \"lm\", formula = \"y ~ x\", se = FALSE, fullrange = TRUE) +\n  theme_classic() +\n  facet_wrap(~ species)\n\n\n\n\n\n\n9.1 fitting model\nWe can use the library lme4 to fit a model of a linear regression with a random effect\n\n\nCode\n# creating log -transformed variables \nMollusca_subset$log_pld &lt;- log(Mollusca_subset$pld)\nMollusca_subset$log_temp &lt;- log(Mollusca_subset$temp)\n\n# mixed model with random intercept only \nRandIntModel_Mollusca &lt;- lmer(log_pld ~ log_temp + (1 | species), data = Mollusca_subset)\n\nsummary(RandIntModel_Mollusca)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: log_pld ~ log_temp + (1 | species)\n   Data: Mollusca_subset\n\nREML criterion at convergence: 43.7\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.37222 -0.32686 -0.09382  0.43821  2.34871 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n species  (Intercept) 0.86457  0.9298  \n Residual             0.03309  0.1819  \nNumber of obs: 44, groups:  species, 16\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   7.1728     0.5284  13.575\nlog_temp     -1.5175     0.1592  -9.531\n\nCorrelation of Fixed Effects:\n         (Intr)\nlog_temp -0.896\n\n\n\nthe fixed effects: section in the output is the estimate for the fixed slope, and the grand mean of the intercept. in the fixed effects section the intercept here is the random effect, and you can see this\nin the Random effects section in this output, in this case our random effect was specified in the the Names sections, which tells us the parameter is the intercept.\nthe Group section tells us we have a random intercept for each species. We also have the variance and standard deviation for the random effects (as well as the residuals)\n\nSince we have a random effect at the individual level, we can subset this section out so it is clear to see that these organism will have a random intercept and fixed slope.\n\n\nCode\n# subset of the coefficients for random intercept and fixed slop\n\ncoef(RandIntModel_Mollusca)$species\n\n\n                      (Intercept) log_temp\nChlamys.hastata          7.612748 -1.51751\nCrassostrea.virginica    7.592515 -1.51751\nCrepidula.fornicata.     8.045609 -1.51751\nCrepidula.plana          8.063220 -1.51751\nHaliotis.asinina         5.807247 -1.51751\nHaliotis.fulgens         6.083069 -1.51751\nHaliotis.sorenseni.      6.688210 -1.51751\nMactra.solidissima       7.589660 -1.51751\nMopalia.muscosa          7.061086 -1.51751\nMytilus.edulis           7.141801 -1.51751\nNassarius.obsoletus      7.370159 -1.51751\nOstrea.lurida            6.967626 -1.51751\nPerna.viridis            8.144314 -1.51751\nStrombus.gigas           8.048942 -1.51751\nTivela.mactroides        7.677603 -1.51751\nTonicella.lineata        4.871674 -1.51751\n\n\nSo now we can see that in the Mollusca subset, we have all random intercepts for individual specifies, but the same slope.\n\n\n9.2 Inter class correlation coefficient (ICC)\nfor a random intercept model, we can run a diagnostic called the inter-class correlation coefficient (ICC), which lets us know how much group specific information is available for the random effect. this is somewhat similar to the ANOVA, in which it looks at the variability within groups compared to the variability between groups. Low ICC means that observation within group don’t really cluster.\n ICC = {\\sigma^2_{\\alpha} \\over \\sigma^2 + \\sigma^2_\\alpha} \n\n\nCode\n# creating data frame\nvar &lt;- as.data.frame(VarCorr(RandIntModel_Mollusca))\n\n#check our data frame\nvar\n\n\n       grp        var1 var2       vcov     sdcor\n1  species (Intercept) &lt;NA&gt; 0.86457132 0.9298233\n2 Residual        &lt;NA&gt; &lt;NA&gt; 0.03309183 0.1819116\n\n\nCode\n#ICC equation\nICC &lt;- var$vcov[1] / (var$vcov[1] + var$vcov[2])\n\n# ICC value \nICC\n\n\n[1] 0.9631356\n\n\nIn our model, the \\sigma^2_{\\alpha} is 0.8645 (also the vcov part) and the \\sigma^2 is 0.033. so once we do the mathematics we get 0.9631. Which is the proportion of the total variance in Y that is accounted for by clustering. This is a high value and therefore, suggesting we have within-group variability, so it might be good we are running this random effects model."
  },
  {
    "objectID": "RandomInterceptModel.html#interpretation-of-results",
    "href": "RandomInterceptModel.html#interpretation-of-results",
    "title": "Random Intercept Model",
    "section": "10 Interpretation of results",
    "text": "10 Interpretation of results\n\n\nCode\nsummary(RandIntModel_Mollusca)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: log_pld ~ log_temp + (1 | species)\n   Data: Mollusca_subset\n\nREML criterion at convergence: 43.7\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.37222 -0.32686 -0.09382  0.43821  2.34871 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n species  (Intercept) 0.86457  0.9298  \n Residual             0.03309  0.1819  \nNumber of obs: 44, groups:  species, 16\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   7.1728     0.5284  13.575\nlog_temp     -1.5175     0.1592  -9.531\n\nCorrelation of Fixed Effects:\n         (Intr)\nlog_temp -0.896\n\n\nInterpretation: Summary of this PLD data includes information about the random effects. Here we can see that the column groups shows the random effect variable. in the name section, you can see that the random effect is our intercept. so we have the variation due to the species. in the Residuals section, this is the variation that cannot be explained by the model (the error). As you will notice our Standard error is smaller compared to the ordinary regression we ran in the previous one. Standard error for this model is 0.15 and the previous standard error for the first model we ran was 0.18.\nSo 0.86 / 0.86 + 0.03 = 0.96 , so the difference between between species can explain 96% of the variance that is is left over after the variance is explained by our fixed effect. since the random effects of the species explain most.\nthere is a very long description on the why the lmer() function doesn’t include the p-value that can be found here.\ninterpretation of temp variable for the fixed part, we can interpret this parameter the same as a single-level regression model, so \\beta_1 is the increase/decrease in response for 1 unit increase/decrease in x. In other words, for one unit increase in the degrees of temperature, there is a -1.5 decrease in Planktonic larval duration. (or, as the temperature increase, the plankton duration is lower.)"
  },
  {
    "objectID": "RandomInterceptModel.html#conclusion",
    "href": "RandomInterceptModel.html#conclusion",
    "title": "Random Intercept Model",
    "section": "11 Conclusion",
    "text": "11 Conclusion\nIn this lecture you learned about the importance of a random intercept model, when it is appropriate to use a random intercept model, the difference between an ordinary single-level model, and a random intercept model, the assumptions of the random intercept model, hypothesis testing for the variation, the Interclass correlation coefficient (ICC) and finally, how to interpret results from the fixed part and the random part of a random intercept model."
  },
  {
    "objectID": "RandomInterceptModel.html#references",
    "href": "RandomInterceptModel.html#references",
    "title": "Random Intercept Model",
    "section": "12 References",
    "text": "12 References\n\nAbedin, Jaynal, and Kishor Kumar Das. 2015. Data Manipulation with r. Packt Publishing Ltd.\nAnnesley, Thomas M. 2010. “Bars and Pies Make Better Desserts Than Figures.” Clinical Chemistry 56 (9): 1394–1400.\nAnscombe, Francis J. 1973. “Graphs in Statistical Analysis.” The American Statistician 27 (1): 17–21.\nBorer, Elizabeth T, Eric W Seabloom, Matthew B Jones, and Mark Schildhauer. 2009. “Some Simple Guidelines for Effective Data Management.” The Bulletin of the Ecological Society of America 90 (2): 205–14.\nBorghi, John, Stephen Abrams, Daniella Lowenberg, Stephanie Simms, and John Chodacki. 2018. “Support Your Data: A Research Data Management Guide for Researchers.” Research Ideas and Outcomes 4: e26439.\nBroman, Karl W, and Kara H Woo. 2018. “Data Organization in Spreadsheets.” The American Statistician 72 (1): 2–10.\nChamberlin, Thomas C. 1890. “The Method of Multiple Working Hypotheses.” Science 15 (366): 92–96.\nsupplementary material for Tom Snijders and Roel Bosker textbook - Shjders, T Bosker R, 1999. Multilevel analysis: an introduction to basic and advanced mltilevel modeling, London, Sage, including updates and corrections data set examples http://stat.gamma.rug.nl/multilevel.htm\nUniversity of Bristol, Random Intercept model, (2018). http://www.bristol.ac.uk/cmm/learning/videos/random-intercepts.html\nMidway, S. (2019). “Data Analysis in R.” https://bookdown.org/steve_midway/DAR/random-effects.html"
  }
]